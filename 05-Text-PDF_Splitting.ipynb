{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JSJeong-me/Retriever/blob/main/05-Text-PDF_Splitting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1Yj7l0HEuYd_",
      "metadata": {
        "id": "1Yj7l0HEuYd_"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install python-dotenv\n",
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WwYxQvutubOj",
      "metadata": {
        "id": "WwYxQvutubOj"
      },
      "outputs": [],
      "source": [
        "!echo \"OPENAI_API_KEY=\" >> .env\n",
        "!source /content/.env\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "# Access the API key using the variable name defined in the .env file\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83NpJtIZu_xb",
      "metadata": {
        "id": "83NpJtIZu_xb"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9663ad9-217c-46d3-befc-8ccabeba890b",
      "metadata": {
        "id": "a9663ad9-217c-46d3-befc-8ccabeba890b"
      },
      "source": [
        "# 5 Levels Of Text Splitting\n",
        "\n",
        "In this tutorial we are reviewing the 5 Levels Of Text Splitting. This is an unofficial list put together for fun and educational purposes.\n",
        "\n",
        "Ever try to put a long piece of text into ChatGPT but it tells you it’s too long? Or you're trying to give your application better long term memory, but it’s still just not quite working.\n",
        "\n",
        "One of the most effective strategies to improve performance of your language model applications is to split your large data into smaller pieces. This is call splitting or chunking (we'll use these terms interchangeably). In the world of multi-modal, splitting also applies to images.\n",
        "\n",
        "We are going to cover a lot, but if you make it to the end, I guarantee you’ll have a solid grasp on chunking theory, strategies, and resources to learn more.\n",
        "\n",
        "**Levels Of Text Splitting**\n",
        "* **Level 1: [Character Splitting](#CharacterSplitting)** - Simple static character chunks of data\n",
        "* **Level 2: [Recursive Character Text Splitting](#RecursiveCharacterSplitting)** - Recursive chunking based on a list of separators\n",
        "* **Level 3: [Document Specific Splitting](#DocumentSpecific)** - Various chunking methods for different document types (PDF, Python, Markdown)\n",
        "* **Level 4: [Semantic Splitting](#SemanticChunking)** - Embedding walk based chunking\n",
        "* **Level 5: [Agentic Splitting](#AgenticChunking)** - Experimental method of splitting text with an agent-like system. Good for if you believe that token cost will trend to $0.00\n",
        "* **\\*Bonus Level:\\*** **[Alternative Representation Chunking + Indexing](#BonusLevel)** - Derivative representations of your raw text that will aid in retrieval and indexing\n",
        "\n",
        "**Notebook resources:**\n",
        "* [Video Overview]() - Walkthrough of this code with commentary\n",
        "* [ChunkViz.com](https://www.chunkviz.com/) - Visual representation of chunk splitting methods\n",
        "* [RAGAS](https://github.com/explodinggradients/ragas) - Retrieval evaluation framework\n",
        "\n",
        "This tutorial was created with ❤️ by [Greg Kamradt](https://twitter.com/GregKamradt). MIT license, attribution is always welcome.\n",
        "\n",
        "This tutorial will use code from LangChain (`pip install langchain`) & Llama Index (`pip install llama-index`)\n",
        "\n",
        "**Evaluations**\n",
        "\n",
        "It's important to test your chunking strategies in retrieval evals. It doesn't matter how you chunk if the performance of your application isn't great.\n",
        "\n",
        "Eval Frameworks:\n",
        "\n",
        "* [LangChain Evals](https://python.langchain.com/docs/guides/evaluation/)\n",
        "* [Llama Index Evals](https://docs.llamaindex.ai/en/stable/module_guides/evaluating/root.html)\n",
        "* [RAGAS Evals](https://github.com/explodinggradients/ragas)\n",
        "\n",
        "I'm not going to demo evals for each method because success is domain specific. The arbitrary eval that I pick may not be suitable for your data. If anyone is interested in collaborating on a rigorous evaluation of different chunking strategies, please reach out (contact@dataindependent.com).\n",
        "\n",
        "If you only walk away from this tutorial with one thing have it be the **The Chunking Commandment**\n",
        "\n",
        "**The Chunking Commandment:** Your goal is not to chunk for chunking sake, our goal is to get our data in a format where it can be retrieved for value later.\n",
        "\n",
        "## Level 1: Character Splitting <a id=\"CharacterSplitting\"></a>\n",
        "Character splitting is the most basic form of splitting up your text. It is the process of simply dividing your text into N-character sized chunks regardless of their content or form.\n",
        "\n",
        "This method isn't recommended for any applications - but it's a great starting point for us to understand the basics.\n",
        "\n",
        "* **Pros:** Easy & Simple\n",
        "* **Cons:** Very rigid and doesn't take into account the structure of your text\n",
        "\n",
        "Concepts to know:\n",
        "* **Chunk Size** - The number of characters you would like in your chunks. 50, 100, 100,000, etc.\n",
        "* **Chunk Overlap** - The amount you would like your sequential chunks to overlap. This is to try to avoid cutting a single piece of context into multiple pieces. This will create duplicate data across chunks.\n",
        "\n",
        "First let's get some sample text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c96299fc-30f5-4edf-ac23-23a29f9c7282",
      "metadata": {
        "id": "c96299fc-30f5-4edf-ac23-23a29f9c7282"
      },
      "outputs": [],
      "source": [
        "text = \"This is the text I would like to chunk up. It is the example text for this exercise\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e1cf67e-98d7-48bd-9867-f72be72e3f4a",
      "metadata": {
        "id": "4e1cf67e-98d7-48bd-9867-f72be72e3f4a"
      },
      "source": [
        "Then let's split this text manually"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f11fb88f-17ed-44c2-b4de-a8a527fe63c7",
      "metadata": {
        "id": "f11fb88f-17ed-44c2-b4de-a8a527fe63c7"
      },
      "outputs": [],
      "source": [
        "# Create a list that will hold your chunks\n",
        "chunks = []\n",
        "\n",
        "chunk_size = 35 # Characters\n",
        "\n",
        "# Run through the a range with the length of your text and iterate every chunk_size you want\n",
        "for i in range(0, len(text), chunk_size):\n",
        "    chunk = text[i:i + chunk_size]\n",
        "    chunks.append(chunk)\n",
        "chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "140085b7-c6af-4003-923c-73feb1825965",
      "metadata": {
        "id": "140085b7-c6af-4003-923c-73feb1825965"
      },
      "source": [
        "Congratulations! You just split your first text. We have long way to go but you're already making progress. Feel like a language model practitioner yet?\n",
        "\n",
        "When working with text in the language model world, we don't deal with raw strings. It is more common to work with documents. Documents are objects that hold the text you're concerned with, but also additional metadata which makes filtering and manipulation easier later.\n",
        "\n",
        "We could convert our list of strings into documents, but I'd rather start from scratch and create the docs.\n",
        "\n",
        "Let's load up LangChains `CharacterSplitter` to do this for us"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d85945f0-4a09-4bd9-bdb6-bafe03089053",
      "metadata": {
        "id": "d85945f0-4a09-4bd9-bdb6-bafe03089053"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff5f3c0a-09c9-45a9-8f47-d28baf22b201",
      "metadata": {
        "id": "ff5f3c0a-09c9-45a9-8f47-d28baf22b201"
      },
      "source": [
        "Then let's load up this text splitter. I need to specify `chunk overlap` and `separator` or else we'll get funk results. We'll get into those next"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dcbeb8d-c5a0-4047-8250-967313c20935",
      "metadata": {
        "id": "3dcbeb8d-c5a0-4047-8250-967313c20935"
      },
      "outputs": [],
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size = 35, chunk_overlap=0, separator='', strip_whitespace=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ae25bbe-d7d1-44da-820b-3cd34a1cfc67",
      "metadata": {
        "id": "5ae25bbe-d7d1-44da-820b-3cd34a1cfc67"
      },
      "source": [
        "Then we can actually split our text via `create_documents`. Note: `create_documents` expects a list of texts, so if you just have a string (like we do) you'll need to wrap it in `[]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afe4945b-ce08-49aa-a5dc-65a0e59922f2",
      "metadata": {
        "id": "afe4945b-ce08-49aa-a5dc-65a0e59922f2"
      },
      "outputs": [],
      "source": [
        "text_splitter.create_documents([text])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "331025f4-6ef4-4459-bcb6-df7824e78ce4",
      "metadata": {
        "id": "331025f4-6ef4-4459-bcb6-df7824e78ce4"
      },
      "source": [
        "Notice how this time we have the same chunks, but they are in documents. These will play nicely with the rest of the LangChain world. Also notice how the trailing whitespace on the end of the 2nd chunk is missing. This is because LangChain removes it, see [this line](https://github.com/langchain-ai/langchain/blob/f36ef0739dbb548cabdb4453e6819fc3d826414f/libs/langchain/langchain/text_splitter.py#L167) for where they do it. You can avoid this with `strip_whitespace=False`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ed0f193-4098-4fb1-a42f-7f96cd182188",
      "metadata": {
        "id": "1ed0f193-4098-4fb1-a42f-7f96cd182188"
      },
      "source": [
        "**Chunk Overlap & Separators**\n",
        "\n",
        "**Chunk overlap** will blend together our chunks so that the tail of Chunk #1 will be the same thing and the head of Chunk #2 and so on and so forth.\n",
        "\n",
        "This time I'll load up my overlap with a value of 4, this means 4 characters of overlap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc66f496-7b0d-4b2a-a43d-e8f06d58c934",
      "metadata": {
        "id": "fc66f496-7b0d-4b2a-a43d-e8f06d58c934"
      },
      "outputs": [],
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size = 35, chunk_overlap=4, separator='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd5d7e36-b592-430e-9069-cc025c78d7ef",
      "metadata": {
        "id": "fd5d7e36-b592-430e-9069-cc025c78d7ef"
      },
      "outputs": [],
      "source": [
        "text_splitter.create_documents([text])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcd4aaa8-b90b-499e-b2d5-bc623b5bb751",
      "metadata": {
        "id": "dcd4aaa8-b90b-499e-b2d5-bc623b5bb751"
      },
      "source": [
        "Notice how we have the same chunks, but now there is overlap between 1 & 2 and 2 & 3. The 'o ch' on the tail of Chunk #1 matches the 'o ch' of the head of Chunk #2.\n",
        "\n",
        "I wanted a better way to visualize this, so I made [ChunkViz.com](www.chunkviz.com) to help show it. Here's what the same text looks like.\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"static/ChunkVizCharacter34_4_w_overlap.png\" alt=\"image\" style=\"max-width: 800px;\">\n",
        "</div>\n",
        "\n",
        "static/ChunkVizCharacterRecursive.png\n",
        "\n",
        "Check out how we have three colors, with two overlaping sections.\n",
        "\n",
        "**Separators** are character(s) sequences you would like to split on. Say you wanted to chunk your data at `ch`, you can specify it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "814ce9aa-17c3-4205-b433-2eae612c2225",
      "metadata": {
        "id": "814ce9aa-17c3-4205-b433-2eae612c2225"
      },
      "outputs": [],
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size = 35, chunk_overlap=0, separator='ch')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb759b1f-dab0-4f5e-a0c0-220374313da6",
      "metadata": {
        "id": "bb759b1f-dab0-4f5e-a0c0-220374313da6"
      },
      "outputs": [],
      "source": [
        "text_splitter.create_documents([text])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72c7ff4a-b6f0-4924-9a9c-2791dddc5b37",
      "metadata": {
        "id": "72c7ff4a-b6f0-4924-9a9c-2791dddc5b37"
      },
      "source": [
        "#### Llama Index\n",
        "\n",
        "[Llama Index](https://www.llamaindex.ai/) is a great choice for flexibility in the chunking and indexing process. They provide node relationships out of the box which can aid in retrieval later.\n",
        "\n",
        "Let's take a look at their sentence splitter. It is similar to the character splitter, but using its default settings, it'll split on sentences instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "624832d6-f3fd-45c4-b52e-bbc86e5e3cd0",
      "metadata": {
        "id": "624832d6-f3fd-45c4-b52e-bbc86e5e3cd0"
      },
      "outputs": [],
      "source": [
        "from llama_index.text_splitter import SentenceSplitter\n",
        "from llama_index import SimpleDirectoryReader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6e77494-a253-4414-b32d-2246fcb396ef",
      "metadata": {
        "id": "b6e77494-a253-4414-b32d-2246fcb396ef"
      },
      "source": [
        "Load up your splitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef9e6627-81a5-463d-9180-5ff2ff1d40f9",
      "metadata": {
        "id": "ef9e6627-81a5-463d-9180-5ff2ff1d40f9"
      },
      "outputs": [],
      "source": [
        "splitter = SentenceSplitter(\n",
        "    chunk_size=200,\n",
        "    chunk_overlap=15,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23214df1-064d-47ad-9058-f9a5f113b6cf",
      "metadata": {
        "id": "23214df1-064d-47ad-9058-f9a5f113b6cf"
      },
      "source": [
        "Load up your document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cafba29-b973-4b37-806f-6f4893eae02e",
      "metadata": {
        "id": "7cafba29-b973-4b37-806f-6f4893eae02e"
      },
      "outputs": [],
      "source": [
        "documents = SimpleDirectoryReader(\n",
        "    input_files=[\"lose-money.txt\"]\n",
        ").load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2ceae08-1f5e-46bc-9d98-e95182fe8c3c",
      "metadata": {
        "id": "d2ceae08-1f5e-46bc-9d98-e95182fe8c3c"
      },
      "source": [
        "Create your nodes. Nodes are similar to documents but with more relationship data added to them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52f9a2e2-3d56-4727-afce-c81c13089324",
      "metadata": {
        "id": "52f9a2e2-3d56-4727-afce-c81c13089324"
      },
      "outputs": [],
      "source": [
        "nodes = splitter.get_nodes_from_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec353aac-ea8f-421a-9895-0af8afdc08e0",
      "metadata": {
        "id": "ec353aac-ea8f-421a-9895-0af8afdc08e0"
      },
      "source": [
        "Then let's take a look at one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "898a421f-54a0-45a4-8acb-c7087b6a883f",
      "metadata": {
        "id": "898a421f-54a0-45a4-8acb-c7087b6a883f"
      },
      "outputs": [],
      "source": [
        "nodes[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "539aa1a2-67b8-4585-b0d3-306703ea856b",
      "metadata": {
        "id": "539aa1a2-67b8-4585-b0d3-306703ea856b"
      },
      "source": [
        "As you can see there is a lot more relationship data held within Llama Index's nodes. We'll talk about those later, I don't want to get ahead of ourselves\n",
        "\n",
        "Basic Character splitting is likely only useful for a few applications, maybe yours!\n",
        "\n",
        "## Level 2: Recursive Character Text Splitting\n",
        "<a id=\"RecursiveCharacterSplitting\"></a>\n",
        "Let's jump a level of complexity.\n",
        "\n",
        "The problem with Level #1 is that we don't take into account the structure of our document at all. We simply split by a fix number of characters.\n",
        "\n",
        "The Recursive Character Text Splitter helps with this. With it, we'll specify a series of separatators which will be used to split our docs.\n",
        "\n",
        "You can see the default separators for LangChain [here](https://github.com/langchain-ai/langchain/blob/9ef2feb6747f5a69d186bd623b569ad722829a5e/libs/langchain/langchain/text_splitter.py#L842). Let's take a look at them one by one.\n",
        "\n",
        "* \"\\n\\n\" - Double new line, or most commonly paragraph breaks\n",
        "* \"\\n\" - New lines\n",
        "* \" \" - Spaces\n",
        "* \"\" - Characters\n",
        "\n",
        "I'm not sure why a period (\".\") isn't included on the list, perhaps it is not universal enough? If you know, let me know.\n",
        "\n",
        "This is the swiss army knife of splitters and my first choice when mocking up a quick application. If you don't know which splitter to start with, this is a good first bet.\n",
        "\n",
        "Let's try it out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49f42bea-3d06-404d-9f8c-f15f7ff7591b",
      "metadata": {
        "id": "49f42bea-3d06-404d-9f8c-f15f7ff7591b"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb6f79f4-769b-474b-8d7d-19cb48407cd6",
      "metadata": {
        "id": "bb6f79f4-769b-474b-8d7d-19cb48407cd6"
      },
      "source": [
        "Then let's load up a larger piece of text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0772695d-0c5e-4e19-bb69-14e9bd7a15a7",
      "metadata": {
        "id": "0772695d-0c5e-4e19-bb69-14e9bd7a15a7"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "One of the most important things I didn't understand about the world when I was a child is the degree to which the returns for performance are superlinear.\n",
        "\n",
        "Teachers and coaches implicitly told us the returns were linear. \"You get out,\" I heard a thousand times, \"what you put in.\" They meant well, but this is rarely true. If your product is only half as good as your competitor's, you don't get half as many customers. You get no customers, and you go out of business.\n",
        "\n",
        "It's obviously true that the returns for performance are superlinear in business. Some think this is a flaw of capitalism, and that if we changed the rules it would stop being true. But superlinear returns for performance are a feature of the world, not an artifact of rules we've invented. We see the same pattern in fame, power, military victories, knowledge, and even benefit to humanity. In all of these, the rich get richer. [1]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fbb158c-6bbe-4f49-95df-a8b43965a566",
      "metadata": {
        "id": "9fbb158c-6bbe-4f49-95df-a8b43965a566"
      },
      "source": [
        "Now let's make our text splitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03ec54c4-bda6-4254-97dd-983775b1d729",
      "metadata": {
        "id": "03ec54c4-bda6-4254-97dd-983775b1d729"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 65, chunk_overlap=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "887c7676-1e67-4084-94d3-59689eb399c6",
      "metadata": {
        "id": "887c7676-1e67-4084-94d3-59689eb399c6"
      },
      "outputs": [],
      "source": [
        "text_splitter.create_documents([text])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fa00043-1655-4113-bb28-f3a998d5713a",
      "metadata": {
        "id": "3fa00043-1655-4113-bb28-f3a998d5713a"
      },
      "source": [
        "Notice how now there are more chunks that end with a period \".\". This is because those likely are the end of a paragraph and the splitter first looks for double new lines (paragraph break).\n",
        "\n",
        "Once paragraphs are split, then it looks at the chunk size, if a chunk is too big, then it'll split by the next separator. If the chunk is still too big, then it'll move onto the next one and so forth.\n",
        "\n",
        "For text of this size, let's split on something bigger."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6da8734e-47da-4a08-8459-9bf8bfed7fe4",
      "metadata": {
        "id": "6da8734e-47da-4a08-8459-9bf8bfed7fe4"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 450, chunk_overlap=0)\n",
        "text_splitter.create_documents([text])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e99768f-8732-44e4-b8d8-cc5ac1fe4661",
      "metadata": {
        "id": "1e99768f-8732-44e4-b8d8-cc5ac1fe4661"
      },
      "source": [
        "For this text, 450 splits the paragraphs perfectly. You can even switch the chunk size to 469 and get the same splits. This is because this splitter builds in a bit of cushion and wiggle room to allow your chunks to 'snap' to the nearest separator.\n",
        "\n",
        "Let's view this visually\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"static/ChunkVizCharacterRecursive.png\" alt=\"image\" style=\"max-width: 800px;\">\n",
        "</div>\n",
        "\n",
        "Wow - you already made it to level 2, awesome! We're on a roll. If you like the content, I send updates to email subscribers on projects I'm working on. If you want to get the scoop, sign up [here](https://mail.gregkamradt.com/signup)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5f32a73-0c8a-498c-a3a1-3e7dba4658c9",
      "metadata": {
        "id": "c5f32a73-0c8a-498c-a3a1-3e7dba4658c9"
      },
      "source": [
        "## Level 3: Document Specific Splitting <a id=\"DocumentSpecific\"></a>\n",
        "\n",
        "Stepping up our levels ladder, let's start to handle document types other than normal prose in a .txt. What if you have pictures? or a PDF? or code snippets?\n",
        "\n",
        "Our first two levels wouldn't work great for this so we'll need to find a different tactic.\n",
        "\n",
        "This level is all about making your chunking strategy fit your different data formats. Let's run through a bunch of examples of this in action\n",
        "\n",
        "The Markdown, Python, and JS splitters will basically be similar to Recursive Character, but with different separators.\n",
        "\n",
        "See all of LangChains document splitters [here](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/code_splitter) and Llama Index ([HTML](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules.html#htmlnodeparser), [JSON](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules.html#jsonnodeparser), [Markdown](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules.html#markdownnodeparser))\n",
        "\n",
        "### Markdown\n",
        "\n",
        "You can see the separators [here](https://github.com/langchain-ai/langchain/blob/9ef2feb6747f5a69d186bd623b569ad722829a5e/libs/langchain/langchain/text_splitter.py#L1175).\n",
        "\n",
        "Separators:\n",
        "* `\\n#{1,6}` - Split by new lines followed by a header (H1 through H6)\n",
        "* ```` ```\\n ```` - Code blocks\n",
        "* `\\n\\\\*\\\\*\\\\*+\\n` - Horizontal Lines\n",
        "* `\\n---+\\n` - Horizontal Lines\n",
        "* `\\n___+\\n` - Horizontal Lines\n",
        "* `\\n\\n` Double new lines\n",
        "* `\\n` - New line\n",
        "* `\" \"` - Spaces\n",
        "* `\"\"` - Character"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "298fe868-0872-4fa9-9146-fa33e9dd5706",
      "metadata": {
        "id": "298fe868-0872-4fa9-9146-fa33e9dd5706"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import MarkdownTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1d390ed-d046-44f9-a492-9760141f7982",
      "metadata": {
        "id": "e1d390ed-d046-44f9-a492-9760141f7982"
      },
      "outputs": [],
      "source": [
        "splitter = MarkdownTextSplitter(chunk_size = 40, chunk_overlap=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ba14168-451b-4e9c-b1d0-d1eac6996ad3",
      "metadata": {
        "id": "1ba14168-451b-4e9c-b1d0-d1eac6996ad3"
      },
      "outputs": [],
      "source": [
        "markdown_text = \"\"\"\n",
        "# Fun in California\n",
        "\n",
        "## Driving\n",
        "\n",
        "Try driving on the 1 down to San Diego\n",
        "\n",
        "### Food\n",
        "\n",
        "Make sure to eat a burrito while you're there\n",
        "\n",
        "## Hiking\n",
        "\n",
        "Go to Yosemite\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15dcf8de-551a-4477-8e68-57c4c50ddbc4",
      "metadata": {
        "id": "15dcf8de-551a-4477-8e68-57c4c50ddbc4"
      },
      "outputs": [],
      "source": [
        "splitter.create_documents([markdown_text])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56591620-ef0c-41c2-b539-35ad676ed20f",
      "metadata": {
        "id": "56591620-ef0c-41c2-b539-35ad676ed20f"
      },
      "source": [
        "Notice how the splits gravitate towards markdown sections. However, it's still not perfect. Check out how there is a chunk with just \"there\" in it. You'll run into this at low-sized chunks.\n",
        "\n",
        "### Python\n",
        "\n",
        "See the python splitters [here](https://github.com/langchain-ai/langchain/blob/9ef2feb6747f5a69d186bd623b569ad722829a5e/libs/langchain/langchain/text_splitter.py#L1069)\n",
        "\n",
        "* `\\nclass` - Classes first\n",
        "* `\\ndef` - Functions next\n",
        "* `\\n\\tdef` - Indented functions\n",
        "* `\\n\\n` - Double New lines\n",
        "* `\\n` - New Lines\n",
        "* `\" \"` - Spaces\n",
        "* `\"\"` - Characters\n",
        "\n",
        "\n",
        "Let's load up our splitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66edcde5-1e96-4b61-8636-8129d31d7850",
      "metadata": {
        "id": "66edcde5-1e96-4b61-8636-8129d31d7850"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import PythonCodeTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2afa8f93-6b07-484f-86ff-9836f5a5fae1",
      "metadata": {
        "id": "2afa8f93-6b07-484f-86ff-9836f5a5fae1"
      },
      "outputs": [],
      "source": [
        "python_text = \"\"\"\n",
        "class Person:\n",
        "  def __init__(self, name, age):\n",
        "    self.name = name\n",
        "    self.age = age\n",
        "\n",
        "p1 = Person(\"John\", 36)\n",
        "\n",
        "for i in range(10):\n",
        "    print (i)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e8fcc85-714d-4b5c-a5ce-a3f30cfb447b",
      "metadata": {
        "id": "6e8fcc85-714d-4b5c-a5ce-a3f30cfb447b"
      },
      "outputs": [],
      "source": [
        "python_splitter = PythonCodeTextSplitter(chunk_size=100, chunk_overlap=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7b6dd89-6bb9-496a-a85d-3f1871ff9cd0",
      "metadata": {
        "id": "a7b6dd89-6bb9-496a-a85d-3f1871ff9cd0"
      },
      "outputs": [],
      "source": [
        "python_splitter.create_documents([python_text])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c004f19-9e67-451e-abdd-b103acce2996",
      "metadata": {
        "id": "6c004f19-9e67-451e-abdd-b103acce2996"
      },
      "source": [
        "Check out how the class stays together in a single document (good), then the rest of the code is in a second document (ok).\n",
        "\n",
        "I needed to play with the chunk size to get a clean result like that. You'll likely need to do the same for yours which is why using evaluations to determine optimal chunk sizes is crucial.\n",
        "\n",
        "### JS\n",
        "\n",
        "Very similar to python. See the separators [here](https://github.com/langchain-ai/langchain/blob/9ef2feb6747f5a69d186bd623b569ad722829a5e/libs/langchain/langchain/text_splitter.py#L983).\n",
        "\n",
        "Separators:\n",
        "* `\\nfunction` - Indicates the beginning of a function declaration\n",
        "* `\\nconst` - Used for declaring constant variables\n",
        "* `\\nlet` - Used for declaring block-scoped variables\n",
        "* `\\nvar` - Used for declaring a variable\n",
        "* `\\nclass` - Indicates the start of a class definition\n",
        "* `\\nif` - Indicates the beginning of an if statement\n",
        "* `\\nfor` - Used for for-loops\n",
        "* `\\nwhile` - Used for while-loops\n",
        "* `\\nswitch` - Used for switch statements\n",
        "* `\\ncase` - Used within switch statements\n",
        "* `\\ndefault` - Also used within switch statements\n",
        "* `\\n\\n` - Indicates a larger separation in text or code\n",
        "* `\\n` - Separates lines of code or text\n",
        "* `\" \"` - Separates words or tokens in the code\n",
        "* `\"\"` - Makes every character a separate element"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5225b66-4d79-455b-92a1-841fa23ccc4f",
      "metadata": {
        "id": "a5225b66-4d79-455b-92a1-841fa23ccc4f"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, Language"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d75090fa-4d22-4348-8452-eb50eafa784d",
      "metadata": {
        "id": "d75090fa-4d22-4348-8452-eb50eafa784d"
      },
      "outputs": [],
      "source": [
        "javascript_text = \"\"\"\n",
        "// Function is called, the return value will end up in x\n",
        "let x = myFunction(4, 3);\n",
        "\n",
        "function myFunction(a, b) {\n",
        "// Function returns the product of a and b\n",
        "  return a * b;\n",
        "}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "909fde28-43ba-4f07-b9ae-04c21db04055",
      "metadata": {
        "id": "909fde28-43ba-4f07-b9ae-04c21db04055"
      },
      "outputs": [],
      "source": [
        "js_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "    language=Language.JS, chunk_size=65, chunk_overlap=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b70d936-bc31-4ecc-b190-6dd8fffdacb9",
      "metadata": {
        "id": "5b70d936-bc31-4ecc-b190-6dd8fffdacb9"
      },
      "outputs": [],
      "source": [
        "js_splitter.create_documents([javascript_text])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fab2014-e705-4ade-87ea-d967a9c01593",
      "metadata": {
        "id": "8fab2014-e705-4ade-87ea-d967a9c01593"
      },
      "source": [
        "### PDFs w/ tables\n",
        "\n",
        "Ok now things will get a bit spicier.\n",
        "\n",
        "PDFs are an extremely common data type for language model work. Often they'll contain tables that contain information.\n",
        "\n",
        "This could be financial data, studies, academic papers, etc.\n",
        "\n",
        "Trying to split tables by a character based separator isn't reliable. We need to try out a different method. For a deep dive on this I recommend checking out [Lance Martin's](https://twitter.com/RLanceMartin) [tutorial](https://twitter.com/RLanceMartin/status/1721942636364456336) w/ LangChain.\n",
        "\n",
        "I'll be going through a text based methods. [Mayo](https://twitter.com/mayowaoshin) has also outlined a GPT-4V method which tries to pulls tables via vision rather than text. You can check out [here](https://twitter.com/mayowaoshin/status/1727399231734886633).\n",
        "\n",
        "A very convenient way to do this is with [Unstructured](https://unstructured.io/), a library dedicated to making your data LLM ready."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "he_5CIjQzTxg",
      "metadata": {
        "id": "he_5CIjQzTxg"
      },
      "outputs": [],
      "source": [
        "! pip install langchain unstructured[all-docs] pydantic lxml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffd078d0-5651-4ab0-b299-b2ed5a4f7cef",
      "metadata": {
        "id": "ffd078d0-5651-4ab0-b299-b2ed5a4f7cef"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "from unstructured.staging.base import elements_to_json"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b174a72b-0d43-4440-9ef8-f5f3aed0c651",
      "metadata": {
        "id": "b174a72b-0d43-4440-9ef8-f5f3aed0c651"
      },
      "source": [
        "Let's load up our PDF and then parition it. This is a PDF from a [Salesforce earning report](https://investor.salesforce.com/financials/default.aspx)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6xShqwzG1Lps",
      "metadata": {
        "id": "6xShqwzG1Lps"
      },
      "source": [
        "https://github.com/FullStackRetrieval-com/RetrievalTutorials/tree/main/static/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TGLU1fk74KPk",
      "metadata": {
        "id": "TGLU1fk74KPk"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZhEF7Ail4pH1",
      "metadata": {
        "id": "ZhEF7Ail4pH1"
      },
      "outputs": [],
      "source": [
        "!mkdir pdfimages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Hakt6TBA4fWm",
      "metadata": {
        "id": "Hakt6TBA4fWm"
      },
      "outputs": [],
      "source": [
        "# !pip install PyPDF2\n",
        "\n",
        "# import PyPDF2\n",
        "\n",
        "# def get_num_pages(pdf_path):\n",
        "#     with open(pdf_path, \"rb\") as file:\n",
        "#         pdf = PyPDF2.PdfFileReader(file)\n",
        "#         return pdf.getNumPages()\n",
        "\n",
        "# pdf_path = \"./SalesforceFinancial.pdf\"\n",
        "# print(get_num_pages(pdf_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QnxuInk66Cyn",
      "metadata": {
        "id": "QnxuInk66Cyn"
      },
      "outputs": [],
      "source": [
        "!apt install tesseract-ocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ae9042d-be6f-411a-8835-bda30ffa0805",
      "metadata": {
        "id": "2ae9042d-be6f-411a-8835-bda30ffa0805"
      },
      "outputs": [],
      "source": [
        "filename = \"SalesforceFinancial.pdf\"   # https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/static/SalesforceFinancial.pdf\n",
        "\n",
        "# Extracts the elements from the PDF\n",
        "elements = partition_pdf(\n",
        "    filename=filename,\n",
        "\n",
        "    # Unstructured Helpers\n",
        "    strategy=\"hi_res\",\n",
        "    infer_table_structure=True,\n",
        "    model_name=\"yolox\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f65ad413-80f9-43da-8fb5-3a32373c3686",
      "metadata": {
        "id": "f65ad413-80f9-43da-8fb5-3a32373c3686"
      },
      "source": [
        "Let's look at our elements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f5ac388-5b4e-4dcd-bf74-84220c8cdff4",
      "metadata": {
        "id": "0f5ac388-5b4e-4dcd-bf74-84220c8cdff4"
      },
      "outputs": [],
      "source": [
        "elements"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a081974-002c-4060-9d0c-2d4a5f270044",
      "metadata": {
        "id": "1a081974-002c-4060-9d0c-2d4a5f270044"
      },
      "source": [
        "These are just unstructured objects, we could look at them all but I want to look at the table it parsed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c5460db-689d-4e7a-a5bc-a10477c4a61e",
      "metadata": {
        "id": "9c5460db-689d-4e7a-a5bc-a10477c4a61e"
      },
      "outputs": [],
      "source": [
        "elements[-4].metadata.text_as_html"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c269535-4fe9-44f2-8e0d-8578d706db7c",
      "metadata": {
        "id": "8c269535-4fe9-44f2-8e0d-8578d706db7c"
      },
      "source": [
        "That table may look messy, but because it's in HTML format, the LLM is able to parse it much more easily than if it was tab or comma separated. You can copy and paste that html into a [html viewer](https://codebeautify.org/htmlviewer) online to see it reconstructed.\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"static/SalesforceFinancialTable.png\" alt=\"image\" style=\"max-width: 800px;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ed1a5fe-245a-4d3c-86aa-03fca86ba5cf",
      "metadata": {
        "id": "1ed1a5fe-245a-4d3c-86aa-03fca86ba5cf"
      },
      "source": [
        "Awesome, Unstructured was able to pull out the tables for us. It's not perfect, but the team is upgrading their toolset all the time.\n",
        "\n",
        "**Important Point:** Later on when we are doing semantic search over our chunks, trying to match on embeddings from the table directly will be difficult. A common practice that developers do is to *summarize* the table after you've extracted it. Then get an embedding of that summary. If the summary embedding matches what you're looking for, then pass the raw table to your LLM.\n",
        "\n",
        "### Multi-Modal (text + images)\n",
        "\n",
        "Next we'll dive into the world of multi-modal text splitting. This is a very active field and best practices are evolving. I'll show you a method that was made popular by [Lance Martin](https://twitter.com/RLanceMartin/status/1713638963255366091) of LangChain. You can check out his source code [here](https://github.com/langchain-ai/langchain/blob/master/cookbook/Semi_structured_and_multi_modal_RAG.ipynb). If you find a method that works better, share it out with the community!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dee2ed53-96c8-4cf3-89d9-681ff5d4552f",
      "metadata": {
        "id": "dee2ed53-96c8-4cf3-89d9-681ff5d4552f"
      },
      "outputs": [],
      "source": [
        "#!pip3 install \"unstructured[all-docs]\"\n",
        "from typing import Any\n",
        "\n",
        "from pydantic import BaseModel\n",
        "from unstructured.partition.pdf import partition_pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea2f20e1-e0fb-4d17-8ec7-faa4276fba23",
      "metadata": {
        "id": "ea2f20e1-e0fb-4d17-8ec7-faa4276fba23"
      },
      "source": [
        "First, let's go get a PDF to work with. This will be from a visual instruction tuning [paper](https://llava-vl.github.io/).\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"static/VisualInstructionSnapshot.png\" alt=\"image\" style=\"max-width: 800px;\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f2d1771-7170-48c2-a614-480d7d2167df",
      "metadata": {
        "id": "3f2d1771-7170-48c2-a614-480d7d2167df"
      },
      "outputs": [],
      "source": [
        "filepath = \"VisualInstruction.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9240942-2702-45ae-8333-8ac9c3e10343",
      "metadata": {
        "id": "d9240942-2702-45ae-8333-8ac9c3e10343"
      },
      "outputs": [],
      "source": [
        "# Get elements\n",
        "raw_pdf_elements = partition_pdf(\n",
        "    filename=filepath,\n",
        "\n",
        "    # Using pdf format to find embedded image blocks\n",
        "    extract_images_in_pdf=True,\n",
        "\n",
        "    # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\n",
        "    # Titles are any sub-section of the document\n",
        "    infer_table_structure=True,\n",
        "\n",
        "    # Post processing to aggregate text once we have the title\n",
        "    chunking_strategy=\"by_title\",\n",
        "    # Chunking params to aggregate text blocks\n",
        "    # Attempt to create a new chunk 3800 chars\n",
        "    # Attempt to keep chunks > 2000 chars\n",
        "    # Hard max on chunks\n",
        "    max_characters=4000,\n",
        "    new_after_n_chars=3800,\n",
        "    combine_text_under_n_chars=2000,\n",
        "    image_output_dir_path=\"./pdfImages/\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c45aab5-5f51-4d67-9ea2-17c5489a5c2a",
      "metadata": {
        "id": "2c45aab5-5f51-4d67-9ea2-17c5489a5c2a"
      },
      "source": [
        "If you head over to `static/pdfImages/` and check out the images that were parsed.\n",
        "\n",
        "But the images don't do anything sitting in a folder, we need to do something with them! Though a bit outside the scope of chunking, let's talk about how to work with these.\n",
        "\n",
        "The common tactics will either use a multi-modal model to generate summaries of the images or use the image itself for your task. Others get embeddings of images (like [CLIP](https://openai.com/research/clip)).\n",
        "\n",
        "Let's generate summaries so you'll be inspired to take this to the next step. We'll use GPT-4V. Check out other models [here](https://platform.openai.com/docs/model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc40dacd-09a0-4ce0-ae8c-87a3910a1408",
      "metadata": {
        "id": "fc40dacd-09a0-4ce0-ae8c-87a3910a1408"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema.messages import HumanMessage\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from PIL import Image\n",
        "import base64\n",
        "import io\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fe7b48f-7da2-430f-a0dd-c8e1766854a3",
      "metadata": {
        "id": "9fe7b48f-7da2-430f-a0dd-c8e1766854a3"
      },
      "source": [
        "We'll be using gpt-4-vision today"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59b1b6d2-4d84-41dc-8698-8be52e6f5bd8",
      "metadata": {
        "id": "59b1b6d2-4d84-41dc-8698-8be52e6f5bd8"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4-vision-preview\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ea28491-e225-4667-8666-3b0541dbf2b7",
      "metadata": {
        "id": "6ea28491-e225-4667-8666-3b0541dbf2b7"
      },
      "source": [
        "I'm creating quick helper function to convert the image from file to base64 so we can pass it to GPT-4V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed1f096a-abd1-4380-af24-6c65074d2420",
      "metadata": {
        "id": "ed1f096a-abd1-4380-af24-6c65074d2420"
      },
      "outputs": [],
      "source": [
        "# Function to convert image to base64\n",
        "def image_to_base64(image_path):\n",
        "    with Image.open(image_path) as image:\n",
        "        buffered = io.BytesIO()\n",
        "        image.save(buffered, format=image.format)\n",
        "        img_str = base64.b64encode(buffered.getvalue())\n",
        "        return img_str.decode('utf-8')\n",
        "\n",
        "image_str = image_to_base64(\"./figures/figure-15-6.jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80becf02-83bd-4560-af3a-dece72259296",
      "metadata": {
        "id": "80becf02-83bd-4560-af3a-dece72259296"
      },
      "source": [
        "Then we can go ahead and pass our image to the LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "638d7f95-d181-4b4a-aeb0-b40367f0f215",
      "metadata": {
        "id": "638d7f95-d181-4b4a-aeb0-b40367f0f215"
      },
      "outputs": [],
      "source": [
        "chat = ChatOpenAI(model=\"gpt-4-vision-preview\",\n",
        "                  max_tokens=1024)\n",
        "\n",
        "msg = chat.invoke(\n",
        "    [\n",
        "        HumanMessage(\n",
        "            content=[\n",
        "                {\"type\": \"text\", \"text\" : \"Please give a summary of the image provided. Be descriptive\"},\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\n",
        "                        \"url\": f\"data:image/jpeg;base64,{image_str}\"\n",
        "                    },\n",
        "                },\n",
        "            ]\n",
        "        )\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67130242-5d72-4304-a705-c9177558a7d4",
      "metadata": {
        "id": "67130242-5d72-4304-a705-c9177558a7d4"
      },
      "source": [
        "Then the summary returned is what we will put into our vectordata base. Then when it comes time to do our retrieval process, we'll use these embeddings for semantic search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a51c2833-ee05-43b2-9cd2-6710d6e73ee8",
      "metadata": {
        "id": "a51c2833-ee05-43b2-9cd2-6710d6e73ee8"
      },
      "outputs": [],
      "source": [
        "msg.content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b129deb0-7b11-48f6-bc3e-210658e9f8e4",
      "metadata": {
        "id": "b129deb0-7b11-48f6-bc3e-210658e9f8e4"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"static/pdfImages/figure-15-6.jpg\" alt=\"image\" style=\"max-width: 800px;\"><br>\n",
        "    <span><i>static/pdfImages/figure-15-6.jpg</i></span>\n",
        "</div>\n",
        "\n",
        "Hm, that seems about right!\n",
        "\n",
        "There are a ton of ways to go about this (check out the bonus section for more) so don't take my word for it - try 'em."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ba7cf0e-ec8e-4115-be32-e49aaf5adccc",
      "metadata": {
        "id": "0ba7cf0e-ec8e-4115-be32-e49aaf5adccc"
      },
      "source": [
        "## Level 4: Semantic Chunking <a id=\"SemanticChunking\"></a>\n",
        "Isn't it weird that we have a global constant for chunk size? Isn't it even weirder that our normal chunking mechanisms don't take into account the actual content?\n",
        "\n",
        "I'm not the only one who thinks so\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "    <img src=\"static/SemanticChunkingtweet.png\" style=\"max-width:50%; height:auto;\"><br>\n",
        "    <span><i><a href=\"https://twitter.com/thesephist/status/1724159343237456248?s=46\">Source</a></i></span>\n",
        "</div>\n",
        "\n",
        "There has to be a better way - let's explore and find out.\n",
        "\n",
        "Embeddings represent the semantic meaning of a string. They don't do much on their own, but when compared to embeddings of other texts you can start to infer the relationship between chunks. I want to lean into this property and explore using embeddings to find clusters of semantically similar texts.\n",
        "\n",
        "The hypothesis is that semantically similar chunks should be held together.\n",
        "\n",
        "I tried a few methods:\n",
        "1) **Heirarchical clustering with positional reward** - I wanted to see how heirarchical clustering of sentence embeddings would do. But because I chose to split on sentences, there was an issue with small short sentences after a long one. You know? (like this last sentenence). They could change the meaning of a chunk, so I added a positional reward and clusters were more likely to form if they were sentences next to each other. This ended up being ok, but tuning the parameters was slow and unoptimal.\n",
        "2) **Find break points between sequential sentences** - Next up I tried a walk method. I started at the first sentence, got the embedding, then compared it to sentence #2, then compared #2 and #3 and so on. I was looking for \"break points\" where embedding distance was large. If it was above a threshold, then I considered it the start of a new semantic section. I originally tried taking embeddings of every sentence, but this turned out to be too noisy. So I ended up taking groups of 3 sentences (a window), then got an embedding, then dropped the first sentence, and added the next one. This worked out a bit better.\n",
        "\n",
        "I'll show method #2 here - It's not perfect by any means, but it's a good starting point for an exploration and I'd love to hear about how you think it could be improved.\n",
        "\n",
        "First, let's load up our essay that we'll run through. I'm just doing a single essay here to keep the tokens down.\n",
        "\n",
        "We'll be using Paul Graham's [MIT essay](https://paulgraham.com/mit.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c5bb388-0534-4ef5-b5d2-322e945a513b",
      "metadata": {
        "id": "7c5bb388-0534-4ef5-b5d2-322e945a513b"
      },
      "outputs": [],
      "source": [
        "with open('./startups.txt') as file:\n",
        "    essay = file.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d18623d-6fc1-4b90-a016-ee79ac28b9ad",
      "metadata": {
        "id": "6d18623d-6fc1-4b90-a016-ee79ac28b9ad"
      },
      "source": [
        "Then I want to split the entire essay into 1-sentence chunks. I'm going to split on \".\" \"?\" and \"!\". There are better ways to do this but this is quick and easy for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dbbfa5a-45a2-4c6e-84f5-d774183717d0",
      "metadata": {
        "id": "1dbbfa5a-45a2-4c6e-84f5-d774183717d0"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Splitting the essay on '.', '?', and '!'\n",
        "single_sentences_list = re.split(r'(?<=[.?!])\\s+', essay)\n",
        "print (f\"{len(single_sentences_list)} senteneces were found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6340df5c-bae9-4f04-a69a-4263c7e912de",
      "metadata": {
        "id": "6340df5c-bae9-4f04-a69a-4263c7e912de"
      },
      "source": [
        "But a list of sentences can be tough to add more data too. I'm going to turn this into a list of dictionaries (`List[dict]`), of which, the sentences will be a key-value. Then we can start to add more data to each sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1bc484a-5d6f-4df5-a1ec-7803908fac95",
      "metadata": {
        "id": "b1bc484a-5d6f-4df5-a1ec-7803908fac95"
      },
      "outputs": [],
      "source": [
        "sentences = [{'sentence': x, 'index' : i} for i, x in enumerate(single_sentences_list)]\n",
        "sentences[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7655fbb4-73a8-43dd-a6b7-565ecf85890a",
      "metadata": {
        "id": "7655fbb4-73a8-43dd-a6b7-565ecf85890a"
      },
      "source": [
        "Great, now that we have our sentences, I want to combine the sentence before and after so that we reduce noise and capture more of the relationships between sequential sentences.\n",
        "\n",
        "Let's create a function so we can use it again. The `buffer_size` is configurable so you can select how big of a window you want. Keep this number in mind for the later steps. I'll just use `buffer_size=1` for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2c5e5ef-bfbe-4680-9761-0ba7615cc645",
      "metadata": {
        "id": "c2c5e5ef-bfbe-4680-9761-0ba7615cc645"
      },
      "outputs": [],
      "source": [
        "def combine_sentences(sentences, buffer_size=1):\n",
        "    # Go through each sentence dict\n",
        "    for i in range(len(sentences)):\n",
        "\n",
        "        # Create a string that will hold the sentences which are joined\n",
        "        combined_sentence = ''\n",
        "\n",
        "        # Add sentences before the current one, based on the buffer size.\n",
        "        for j in range(i - buffer_size, i):\n",
        "            # Check if the index j is not negative (to avoid index out of range like on the first one)\n",
        "            if j >= 0:\n",
        "                # Add the sentence at index j to the combined_sentence string\n",
        "                combined_sentence += sentences[j]['sentence'] + ' '\n",
        "\n",
        "        # Add the current sentence\n",
        "        combined_sentence += sentences[i]['sentence']\n",
        "\n",
        "        # Add sentences after the current one, based on the buffer size\n",
        "        for j in range(i + 1, i + 1 + buffer_size):\n",
        "            # Check if the index j is within the range of the sentences list\n",
        "            if j < len(sentences):\n",
        "                # Add the sentence at index j to the combined_sentence string\n",
        "                combined_sentence += ' ' + sentences[j]['sentence']\n",
        "\n",
        "        # Then add the whole thing to your dict\n",
        "        # Store the combined sentence in the current sentence dict\n",
        "        sentences[i]['combined_sentence'] = combined_sentence\n",
        "\n",
        "    return sentences\n",
        "\n",
        "sentences = combine_sentences(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0c06ebf-af2c-4f14-924d-e0d0cab05f09",
      "metadata": {
        "id": "a0c06ebf-af2c-4f14-924d-e0d0cab05f09"
      },
      "outputs": [],
      "source": [
        "sentences[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0deb04a6-d0ad-4757-af8c-d6d600794357",
      "metadata": {
        "id": "0deb04a6-d0ad-4757-af8c-d6d600794357"
      },
      "source": [
        "Check out how the 2nd sentence (index #1) has the first sentence and 3rd sentence in its `combined_sentence` key now.\n",
        "\n",
        "Now I want to get embeddings for the combined sentences, so we can get the distances between the groups of 3 and find breakpoints. I'll use OpenAI's embeddings for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d757106f-d6cd-4504-96f1-4fe1f82e0b7e",
      "metadata": {
        "id": "d757106f-d6cd-4504-96f1-4fe1f82e0b7e"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "oaiembeds = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d5da984-67dc-43ce-b871-b5e91186767a",
      "metadata": {
        "id": "9d5da984-67dc-43ce-b871-b5e91186767a"
      },
      "source": [
        "Now let's go get our embeddings. We'll do this in batch to make it quicker."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a18ba5b-4c9f-42d8-bad0-ca0825715f1b",
      "metadata": {
        "id": "8a18ba5b-4c9f-42d8-bad0-ca0825715f1b"
      },
      "outputs": [],
      "source": [
        "embeddings = oaiembeds.embed_documents([x['combined_sentence'] for x in sentences])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a2cdc46-ad21-4e8e-a540-548aa3604212",
      "metadata": {
        "id": "6a2cdc46-ad21-4e8e-a540-548aa3604212"
      },
      "source": [
        "Now we have a list of embeddings, but we need to add them to our list of dicts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45828ce9-31cf-4810-a5e8-2098201a91dd",
      "metadata": {
        "id": "45828ce9-31cf-4810-a5e8-2098201a91dd"
      },
      "outputs": [],
      "source": [
        "for i, sentence in enumerate(sentences):\n",
        "    sentence['combined_sentence_embedding'] = embeddings[i]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a4f5f3e-f436-4374-be90-a1f5f819e518",
      "metadata": {
        "id": "7a4f5f3e-f436-4374-be90-a1f5f819e518"
      },
      "source": [
        "Great, now we're getting to the cool part, let's check out the cosine distances between sequential embedding pairs to see where the break points are. We'll add 'distance_to_next' as another key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95792785-a966-4b40-a9c4-c76eaed71766",
      "metadata": {
        "id": "95792785-a966-4b40-a9c4-c76eaed71766"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def calculate_cosine_distances(sentences):\n",
        "    distances = []\n",
        "    for i in range(len(sentences) - 1):\n",
        "        embedding_current = sentences[i]['combined_sentence_embedding']\n",
        "        embedding_next = sentences[i + 1]['combined_sentence_embedding']\n",
        "\n",
        "        # Calculate cosine similarity\n",
        "        similarity = cosine_similarity([embedding_current], [embedding_next])[0][0]\n",
        "\n",
        "        # Convert to cosine distance\n",
        "        distance = 1 - similarity\n",
        "\n",
        "        # Append cosine distance to the list\n",
        "        distances.append(distance)\n",
        "\n",
        "        # Store distance in the dictionary\n",
        "        sentences[i]['distance_to_next'] = distance\n",
        "\n",
        "    # Optionally handle the last sentence\n",
        "    # sentences[-1]['distance_to_next'] = None  # or a default value\n",
        "\n",
        "    return distances, sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45abeb2e-605b-4556-82ca-b7c99eec742a",
      "metadata": {
        "id": "45abeb2e-605b-4556-82ca-b7c99eec742a"
      },
      "source": [
        "Great, now let's pull out the distances from our sentences and then add them as well"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "336bf78a-8109-4fc5-92b3-28aa049b9f99",
      "metadata": {
        "id": "336bf78a-8109-4fc5-92b3-28aa049b9f99"
      },
      "outputs": [],
      "source": [
        "distances, sentences = calculate_cosine_distances(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57eeacef-1594-41ea-a5f9-70f99b8878f3",
      "metadata": {
        "id": "57eeacef-1594-41ea-a5f9-70f99b8878f3"
      },
      "source": [
        "Let's take a look at what our distances array looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c330493-20f9-456e-a6f7-7e16cac4f3d1",
      "metadata": {
        "id": "3c330493-20f9-456e-a6f7-7e16cac4f3d1"
      },
      "outputs": [],
      "source": [
        "distances[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e960090-8714-4481-adec-33fe39c31bc4",
      "metadata": {
        "id": "7e960090-8714-4481-adec-33fe39c31bc4"
      },
      "source": [
        "Hm, yep, just a bunch of numbers that aren't fun to look at. Let's plot them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "986e4885-b079-4a4a-9ae4-a1227d6ce1bc",
      "metadata": {
        "id": "986e4885-b079-4a4a-9ae4-a1227d6ce1bc"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(distances);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4703f6b-9bf1-45ef-b0ea-a4fb95cc9007",
      "metadata": {
        "id": "f4703f6b-9bf1-45ef-b0ea-a4fb95cc9007"
      },
      "source": [
        "Hm, cool! It's interesting to see sections where distances are smaller and then areas of larger distances. What stands out to me most is the outliers which are spread out.\n",
        "\n",
        "There are many ways to chunk up the essay based off these distances, but I'm going to consider any distance above the 95th percentile of distances as a break point. This is the only parameter we'll need to config.\n",
        "\n",
        "I'm going to build in the final viz, check out the video for an iterative build and an overview."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5044cd49-ef33-4d00-a629-bc3411b7939e",
      "metadata": {
        "id": "5044cd49-ef33-4d00-a629-bc3411b7939e"
      },
      "source": [
        "Let's look at the chunks that came out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cba11136-a6fb-4ec6-83db-0476c9efc5e0",
      "metadata": {
        "id": "cba11136-a6fb-4ec6-83db-0476c9efc5e0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "plt.plot(distances);\n",
        "\n",
        "y_upper_bound = .2\n",
        "plt.ylim(0, y_upper_bound)\n",
        "plt.xlim(0, len(distances))\n",
        "\n",
        "# We need to get the distance threshold that we'll consider an outlier\n",
        "# We'll use numpy .percentile() for this\n",
        "breakpoint_percentile_threshold = 95\n",
        "breakpoint_distance_threshold = np.percentile(distances, breakpoint_percentile_threshold) # If you want more chunks, lower the percentile cutoff\n",
        "plt.axhline(y=breakpoint_distance_threshold, color='r', linestyle='-');\n",
        "\n",
        "# Then we'll see how many distances are actually above this one\n",
        "num_distances_above_theshold = len([x for x in distances if x > breakpoint_distance_threshold]) # The amount of distances above your threshold\n",
        "plt.text(x=(len(distances)*.01), y=y_upper_bound/50, s=f\"{num_distances_above_theshold + 1} Chunks\");\n",
        "\n",
        "# Then we'll get the index of the distances that are above the threshold. This will tell us where we should split our text\n",
        "indices_above_thresh = [i for i, x in enumerate(distances) if x > breakpoint_distance_threshold] # The indices of those breakpoints on your list\n",
        "\n",
        "# Start of the shading and text\n",
        "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
        "for i, breakpoint_index in enumerate(indices_above_thresh):\n",
        "    start_index = 0 if i == 0 else indices_above_thresh[i - 1]\n",
        "    end_index = breakpoint_index if i < len(indices_above_thresh) - 1 else len(distances)\n",
        "\n",
        "    plt.axvspan(start_index, end_index, facecolor=colors[i % len(colors)], alpha=0.25)\n",
        "    plt.text(x=np.average([start_index, end_index]),\n",
        "             y=breakpoint_distance_threshold + (y_upper_bound)/ 20,\n",
        "             s=f\"Chunk #{i}\", horizontalalignment='center',\n",
        "             rotation='vertical')\n",
        "\n",
        "# # Additional step to shade from the last breakpoint to the end of the dataset\n",
        "if indices_above_thresh:\n",
        "    last_breakpoint = indices_above_thresh[-1]\n",
        "    if last_breakpoint < len(distances):\n",
        "        plt.axvspan(last_breakpoint, len(distances), facecolor=colors[len(indices_above_thresh) % len(colors)], alpha=0.25)\n",
        "        plt.text(x=np.average([last_breakpoint, len(distances)]),\n",
        "                 y=breakpoint_distance_threshold + (y_upper_bound)/ 20,\n",
        "                 s=f\"Chunk #{i+1}\",\n",
        "                 rotation='vertical')\n",
        "\n",
        "plt.title(\"PG Essay Chunks Based On Embedding Breakpoints\")\n",
        "plt.xlabel(\"Index of sentences in essay (Sentence Position)\")\n",
        "plt.ylabel(\"Cosine distance between sequential sentences\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c942fe5-cfb4-44b2-8177-35983e88ea82",
      "metadata": {
        "id": "6c942fe5-cfb4-44b2-8177-35983e88ea82"
      },
      "source": [
        "Well now that we've succefully distracted ourselves with a visualization, now we need to combine the sentences into chunks.\n",
        "\n",
        "Because we have our breakpoints [23, 40, 51...] I want to make the first chunk 0-22, since the distance jumped on sentence 23."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f17828f0-4d1b-4514-ad87-343301cb8450",
      "metadata": {
        "id": "f17828f0-4d1b-4514-ad87-343301cb8450"
      },
      "outputs": [],
      "source": [
        "# Initialize the start index\n",
        "start_index = 0\n",
        "\n",
        "# Create a list to hold the grouped sentences\n",
        "chunks = []\n",
        "\n",
        "# Iterate through the breakpoints to slice the sentences\n",
        "for index in indices_above_thresh:\n",
        "    # The end index is the current breakpoint\n",
        "    end_index = index\n",
        "\n",
        "    # Slice the sentence_dicts from the current start index to the end index\n",
        "    group = sentences[start_index:end_index + 1]\n",
        "    combined_text = ' '.join([d['sentence'] for d in group])\n",
        "    chunks.append(combined_text)\n",
        "\n",
        "    # Update the start index for the next group\n",
        "    start_index = index + 1\n",
        "\n",
        "# The last group, if any sentences remain\n",
        "if start_index < len(sentences):\n",
        "    combined_text = ' '.join([d['sentence'] for d in sentences[start_index:]])\n",
        "    chunks.append(combined_text)\n",
        "\n",
        "# grouped_sentences now contains the chunked sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8418f022-d929-427e-9977-ec12c4eb0236",
      "metadata": {
        "id": "8418f022-d929-427e-9977-ec12c4eb0236"
      },
      "source": [
        "Great now let's manually inspect a few to make sure they look ok."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d709135b-6465-4720-8eb8-60dc9f91efac",
      "metadata": {
        "id": "d709135b-6465-4720-8eb8-60dc9f91efac"
      },
      "outputs": [],
      "source": [
        "for i, chunk in enumerate(chunks[:2]):\n",
        "    buffer = 200\n",
        "\n",
        "    print (f\"Chunk #{i}\")\n",
        "    print (chunk[:buffer].strip())\n",
        "    print (\"...\")\n",
        "    print (chunk[-buffer:].strip())\n",
        "    print (\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4a86473-b267-483d-ac45-e0f4d30b67e6",
      "metadata": {
        "id": "c4a86473-b267-483d-ac45-e0f4d30b67e6"
      },
      "source": [
        "I want to re-emphasize that this is an exploration of a method that is far from usable yet. This method should be tested with RAG eval to ensure that it works for your use case.\n",
        "\n",
        "I didn't worry about chunk size or overlap with this method, but you could recursively split large chunks if you needed to.\n",
        "\n",
        "How should it be improved? Let me [know](https://twitter.com/GregKamradt)! See me tease this [here](https://twitter.com/GregKamradt/status/1737921395974430953)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c43ff201-d657-49e0-8a03-8e6460113393",
      "metadata": {
        "id": "c43ff201-d657-49e0-8a03-8e6460113393"
      },
      "source": [
        "## Level 5: Agentic Chunking <a id=\"AgenticChunking\"></a>\n",
        "Taking level 4 even further - can we instruct an LLM to do this task like a human would?\n",
        "\n",
        "How does a human even go about chunking in the first place?\n",
        "\n",
        "Well...let me think, how would I go about chunking a document into its discrete parts such that the results were semantically similar?\n",
        "\n",
        "1. I would get myself a scratch piece of paper or notepad\n",
        "2. I'd start at the top of the essay and assume the first part will be a chunk (since we don't have any yet)\n",
        "3. Then I would keep going down the essay and evaluate if a new sentence or piece of the essay should be a part of the first chunk, if not, then create a new one\n",
        "4. Then keep doing that all the way down the essay until we got to the end.\n",
        "\n",
        "Woah! Wait a minute - this is pseudo code for something we can try out. See me tease this [here](https://twitter.com/GregKamradt/status/1738276097471754735).\n",
        "\n",
        "I debated whether or not to hold myself to the strict standard of using the *raw text* from a document, or use a derived form. The former felt like I was being too harsh, so I decided to explore using [propositions](https://twitter.com/LangChainAI/status/1735708004618764470). This is a cool concept ([research paper](https://arxiv.org/pdf/2312.06648.pdf)) that extracts stand alone statements from a raw piece of text.\n",
        "\n",
        "Example: `Greg went to the park. He likes walking` > `['Greg went to the park.', 'Greg likes walking']`\n",
        "\n",
        "Let's do it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a6f79c0-f6bd-4561-bf0a-434955d875e5",
      "metadata": {
        "id": "5a6f79c0-f6bd-4561-bf0a-434955d875e5"
      },
      "outputs": [],
      "source": [
        "from langchain.output_parsers.openai_tools import JsonOutputToolsParser\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain.chains import create_extraction_chain\n",
        "from typing import Optional, List\n",
        "from langchain.chains import create_extraction_chain_pydantic\n",
        "from langchain_core.pydantic_v1 import BaseModel\n",
        "from langchain import hub"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ba10ec8-8227-4b95-a2cc-4149941dfe98",
      "metadata": {
        "id": "3ba10ec8-8227-4b95-a2cc-4149941dfe98"
      },
      "source": [
        "Pulling out propositions is done via a well-crafted prompt. I'm going to pull it from [LangHub](https://smith.langchain.com/hub?organizationId=50995362-9ea0-4378-ad97-b4edae2f9f22), LangChain's home for prompts.\n",
        "\n",
        "You can view the proposition prompt [here](https://smith.langchain.com/hub/wfh/proposal-indexing?organizationId=50995362-9ea0-4378-ad97-b4edae2f9f22).\n",
        "\n",
        "I'll use gpt-4 as the LLM because we aren't messing around. I care more about performance than I do speed or cost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "m99QYuJ0CYz2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m99QYuJ0CYz2",
        "outputId": "57510d64-a71c-4dfc-999f-30c1ede5cee5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchainhub\n",
            "  Downloading langchainhub-0.1.14-py3-none-any.whl (3.4 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchainhub) (2.31.0)\n",
            "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
            "  Downloading types_requests-2.31.0.20240106-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (2023.11.17)\n",
            "Installing collected packages: types-requests, langchainhub\n",
            "Successfully installed langchainhub-0.1.14 types-requests-2.31.0.20240106\n"
          ]
        }
      ],
      "source": [
        "!pip install langchainhub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "6de38943-7697-413e-a2e8-8a62cda914b8",
      "metadata": {
        "id": "6de38943-7697-413e-a2e8-8a62cda914b8"
      },
      "outputs": [],
      "source": [
        "obj = hub.pull(\"wfh/proposal-indexing\")\n",
        "llm = ChatOpenAI(model='gpt-4-1106-preview', openai_api_key = os.getenv(\"OPENAI_API_KEY\", 'YouKey'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a838bc4-e732-492d-80a5-8c829ca637ef",
      "metadata": {
        "id": "5a838bc4-e732-492d-80a5-8c829ca637ef"
      },
      "source": [
        "Then I'll make a runnable w/ langchain, this'll be a short way to combine the prompt and llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "97236eb3-361e-4ebd-aa13-979ac8718f5f",
      "metadata": {
        "id": "97236eb3-361e-4ebd-aa13-979ac8718f5f"
      },
      "outputs": [],
      "source": [
        "# use it in a runnable\n",
        "runnable = obj | llm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bea6b054-bac6-4ce6-be8f-724a6ea1c5ef",
      "metadata": {
        "id": "bea6b054-bac6-4ce6-be8f-724a6ea1c5ef"
      },
      "source": [
        "The output from a runnable is a json-esque structure in a string. We need to pull the sentences out. I found that LangChain's example extraction was giving me a hard time so I'm doing it manually with a pydantic data class. There is definitely room to improve this.\n",
        "\n",
        "Create your class then put it in an extraction chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "10116bfe-12a0-4f8c-95bf-67a0f8c1ceb5",
      "metadata": {
        "id": "10116bfe-12a0-4f8c-95bf-67a0f8c1ceb5"
      },
      "outputs": [],
      "source": [
        "# Pydantic data class\n",
        "class Sentences(BaseModel):\n",
        "    sentences: List[str]\n",
        "\n",
        "# Extraction\n",
        "extraction_chain = create_extraction_chain_pydantic(pydantic_schema=Sentences, llm=llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a462b5c8-5cb2-4dcb-bedd-6ee1ce2cfd08",
      "metadata": {
        "id": "a462b5c8-5cb2-4dcb-bedd-6ee1ce2cfd08"
      },
      "source": [
        "Then wrap it together in a function that'll return a list of propositions to us"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "7ec04652-19e3-4cab-a882-7c31cd87fe1a",
      "metadata": {
        "id": "7ec04652-19e3-4cab-a882-7c31cd87fe1a"
      },
      "outputs": [],
      "source": [
        "def get_propositions(text):\n",
        "    runnable_output = runnable.invoke({\n",
        "    \t\"input\": text\n",
        "    }).content\n",
        "\n",
        "    propositions = extraction_chain.run(runnable_output)[0].sentences\n",
        "    return propositions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de0bf288-24ba-4a50-a225-400c6fff5c61",
      "metadata": {
        "id": "de0bf288-24ba-4a50-a225-400c6fff5c61"
      },
      "source": [
        "Go get your text of choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "2be92b95-76d6-4a2c-ba2f-67379998956e",
      "metadata": {
        "id": "2be92b95-76d6-4a2c-ba2f-67379998956e"
      },
      "outputs": [],
      "source": [
        "with open('./Super-linear-return.txt') as file:\n",
        "    essay = file.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25f4ee69-a87e-481e-b01d-403021a2610b",
      "metadata": {
        "id": "25f4ee69-a87e-481e-b01d-403021a2610b"
      },
      "source": [
        "Then you need to decide what you send to your proposal maker. The prompt has an example that is about 1K characters long. So I would experiment with what works for you. This isn't another chunking decision, just pick something reasonable and try it out.\n",
        "\n",
        "I'm using paragraphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "1121baaf-c65a-4dce-a942-83cfbf6891cc",
      "metadata": {
        "id": "1121baaf-c65a-4dce-a942-83cfbf6891cc"
      },
      "outputs": [],
      "source": [
        "paragraphs = essay.split(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e29ae4f7-e58b-4941-b243-ccc72c3bf37c",
      "metadata": {
        "id": "e29ae4f7-e58b-4941-b243-ccc72c3bf37c"
      },
      "source": [
        "Let's see how many we have"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "6a2c36b2-bce3-461f-9f52-af1c10b1c3ab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a2c36b2-bce3-461f-9f52-af1c10b1c3ab",
        "outputId": "f3dca212-3bce-4c3c-f069-e47b8a3cc23b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "66"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(paragraphs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c3cb285-abf3-4334-aff7-5f84d741c7f4",
      "metadata": {
        "id": "6c3cb285-abf3-4334-aff7-5f84d741c7f4"
      },
      "source": [
        "That's too many for a demo, I'll do just the first couple to show it off."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "46fddfc4-7cd6-4fe4-add7-7a12c593ef6c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46fddfc4-7cd6-4fe4-add7-7a12c593ef6c",
        "outputId": "641041da-e007-48e8-8893-380330155726"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done with 0\n",
            "Done with 1\n",
            "Done with 2\n",
            "Done with 3\n",
            "Done with 4\n"
          ]
        }
      ],
      "source": [
        "essay_propositions = []\n",
        "\n",
        "for i, para in enumerate(paragraphs[:5]):\n",
        "    propositions = get_propositions(para)\n",
        "\n",
        "    essay_propositions.extend(propositions)\n",
        "    print (f\"Done with {i}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c01906ad-b5a2-4b73-a079-4564b66b147e",
      "metadata": {
        "id": "c01906ad-b5a2-4b73-a079-4564b66b147e"
      },
      "source": [
        "Let's take a look at what the propositions look like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "4867944e-13f0-4207-b787-692eeebadea6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4867944e-13f0-4207-b787-692eeebadea6",
        "outputId": "c3ba4229-336b-4e67-ee8c-e7d11b63a5b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You have 23 propositions\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['The return is super linear.',\n",
              " 'The month is October.',\n",
              " 'The year is 2023.',\n",
              " 'I did not understand one important thing about the world when I was a child.',\n",
              " 'The returns for performance are superlinear.',\n",
              " 'Teachers and coaches implicitly told us the returns were linear.',\n",
              " 'Teachers and coaches meant well.',\n",
              " \"The statement 'You get out what you put in' was heard a thousand times by the speaker.\",\n",
              " 'The statement that returns were linear is rarely true.',\n",
              " \"If your product is only half as good as your competitor's product, you don't get half as many customers.\"]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print (f\"You have {len(essay_propositions)} propositions\")\n",
        "essay_propositions[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "941ebfc6-da6a-4fb8-833d-ed2e9810f25e",
      "metadata": {
        "id": "941ebfc6-da6a-4fb8-833d-ed2e9810f25e"
      },
      "source": [
        "So you'll see that they look like regular sentences, but they are actually statements that are able to stand on their own. For example, one of the sentences in the raw text is \"They meant well, but this is rarely true.\" if you were to chunk that on it's own, the LLM would have no idea who you're talking about. Who meant well? What is rarely true? But those have been covered by the propositions.\n",
        "\n",
        "Now onto the cool part, we need a system that can reason about each proposition and determine whether or not it should be a part of an existing chunk or if a new chunk should be made.\n",
        "\n",
        "The pseudo code for how this works is above - I also review this code in the video so make sure to go watch that if you want to see me chat about it live.\n",
        "\n",
        "The script is also in this repo if you've cloned it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "tfyspSsZEmlv",
      "metadata": {
        "id": "tfyspSsZEmlv"
      },
      "outputs": [],
      "source": [
        "!mkdir agentic_chunker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "lzCNyDibE0V0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzCNyDibE0V0",
        "outputId": "73fda507-7b00-453d-ee44-5bff668c58a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Adding: 'The month is October.'\n",
            "No chunks, creating a new one\n",
            "Created new chunk (38538): Date & Times\n",
            "\n",
            "Adding: 'The year is 2023.'\n",
            "Chunk Found (38538), adding to: Date & Times\n",
            "\n",
            "Adding: 'One of the most important things that I didn't understand about the world as a child was the degree to which the returns for performance are superlinear.'\n",
            "No chunks found\n",
            "Created new chunk (25d00): Performance & Rewards Relationship\n",
            "\n",
            "Adding: 'Teachers and coaches implicitly told us that the returns were linear.'\n",
            "No chunks found\n",
            "Created new chunk (580f2): Linear Returns Concept\n",
            "\n",
            "Adding: 'I heard a thousand times that 'You get out what you put in.''\n",
            "Chunk Found (25d00), adding to: Performance & Rewards Relationship\n",
            "\n",
            "You have 3 chunks\n",
            "\n",
            "Chunk #0\n",
            "Chunk ID: 38538\n",
            "Summary: This chunk contains information about specific dates and times, including months and years.\n",
            "Propositions:\n",
            "    -The month is October.\n",
            "    -The year is 2023.\n",
            "\n",
            "\n",
            "\n",
            "Chunk #1\n",
            "Chunk ID: 25d00\n",
            "Summary: This chunk discusses the concept of effort and its disproportionate relationship to rewards and outcomes.\n",
            "Propositions:\n",
            "    -One of the most important things that I didn't understand about the world as a child was the degree to which the returns for performance are superlinear.\n",
            "    -I heard a thousand times that 'You get out what you put in.'\n",
            "\n",
            "\n",
            "\n",
            "Chunk #2\n",
            "Chunk ID: 580f2\n",
            "Summary: This chunk discusses the concept of linear returns as communicated by teachers and coaches.\n",
            "Propositions:\n",
            "    -Teachers and coaches implicitly told us that the returns were linear.\n",
            "\n",
            "\n",
            "\n",
            "Chunk Outline\n",
            "\n",
            "Chunk (38538): Dates & Years\n",
            "Summary: This chunk contains information about specific dates and times, including months and years.\n",
            "\n",
            "Chunk (25d00): Effort & Outcome Disproportionality\n",
            "Summary: This chunk discusses the concept of effort and its disproportionate relationship to rewards and outcomes.\n",
            "\n",
            "Chunk (580f2): Linear Returns Concept\n",
            "Summary: This chunk discusses the concept of linear returns as communicated by teachers and coaches.\n",
            "\n",
            "\n",
            "['The month is October. The year is 2023.', \"One of the most important things that I didn't understand about the world as a child was the degree to which the returns for performance are superlinear. I heard a thousand times that 'You get out what you put in.'\", 'Teachers and coaches implicitly told us that the returns were linear.']\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "import uuid\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "import os\n",
        "from typing import Optional\n",
        "from langchain_core.pydantic_v1 import BaseModel\n",
        "from langchain.chains import create_extraction_chain_pydantic\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "class AgenticChunker:\n",
        "    def __init__(self, openai_api_key=None):\n",
        "        self.chunks = {}\n",
        "        self.id_truncate_limit = 5\n",
        "\n",
        "        # Whether or not to update/refine summaries and titles as you get new information\n",
        "        self.generate_new_metadata_ind = True\n",
        "        self.print_logging = True\n",
        "\n",
        "        if openai_api_key is None:\n",
        "            openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "        if openai_api_key is None:\n",
        "            raise ValueError(\"API key is not provided and not found in environment variables\")\n",
        "\n",
        "        self.llm = ChatOpenAI(model='gpt-4-1106-preview', openai_api_key=openai_api_key, temperature=0)\n",
        "\n",
        "    def add_propositions(self, propositions):\n",
        "        for proposition in propositions:\n",
        "            self.add_proposition(proposition)\n",
        "\n",
        "    def add_proposition(self, proposition):\n",
        "        if self.print_logging:\n",
        "            print (f\"\\nAdding: '{proposition}'\")\n",
        "\n",
        "        # If it's your first chunk, just make a new chunk and don't check for others\n",
        "        if len(self.chunks) == 0:\n",
        "            if self.print_logging:\n",
        "                print (\"No chunks, creating a new one\")\n",
        "            self._create_new_chunk(proposition)\n",
        "            return\n",
        "\n",
        "        chunk_id = self._find_relevant_chunk(proposition)\n",
        "\n",
        "        # If a chunk was found then add the proposition to it\n",
        "        if chunk_id:\n",
        "            if self.print_logging:\n",
        "                print (f\"Chunk Found ({self.chunks[chunk_id]['chunk_id']}), adding to: {self.chunks[chunk_id]['title']}\")\n",
        "            self.add_proposition_to_chunk(chunk_id, proposition)\n",
        "            return\n",
        "        else:\n",
        "            if self.print_logging:\n",
        "                print (\"No chunks found\")\n",
        "            # If a chunk wasn't found, then create a new one\n",
        "            self._create_new_chunk(proposition)\n",
        "\n",
        "\n",
        "    def add_proposition_to_chunk(self, chunk_id, proposition):\n",
        "        # Add then\n",
        "        self.chunks[chunk_id]['propositions'].append(proposition)\n",
        "\n",
        "        # Then grab a new summary\n",
        "        if self.generate_new_metadata_ind:\n",
        "            self.chunks[chunk_id]['summary'] = self._update_chunk_summary(self.chunks[chunk_id])\n",
        "            self.chunks[chunk_id]['title'] = self._update_chunk_title(self.chunks[chunk_id])\n",
        "\n",
        "    def _update_chunk_summary(self, chunk):\n",
        "        \"\"\"\n",
        "        If you add a new proposition to a chunk, you may want to update the summary or else they could get stale\n",
        "        \"\"\"\n",
        "        PROMPT = ChatPromptTemplate.from_messages(\n",
        "            [\n",
        "                (\n",
        "                    \"system\",\n",
        "                    \"\"\"\n",
        "                    You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
        "                    A new proposition was just added to one of your chunks, you should generate a very brief 1-sentence summary which will inform viewers what a chunk group is about.\n",
        "\n",
        "                    A good summary will say what the chunk is about, and give any clarifying instructions on what to add to the chunk.\n",
        "\n",
        "                    You will be given a group of propositions which are in the chunk and the chunks current summary.\n",
        "\n",
        "                    Your summaries should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
        "                    Or month, generalize it to \"date and times\".\n",
        "\n",
        "                    Example:\n",
        "                    Input: Proposition: Greg likes to eat pizza\n",
        "                    Output: This chunk contains information about the types of food Greg likes to eat.\n",
        "\n",
        "                    Only respond with the chunk new summary, nothing else.\n",
        "                    \"\"\",\n",
        "                ),\n",
        "                (\"user\", \"Chunk's propositions:\\n{proposition}\\n\\nCurrent chunk summary:\\n{current_summary}\"),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        runnable = PROMPT | self.llm\n",
        "\n",
        "        new_chunk_summary = runnable.invoke({\n",
        "            \"proposition\": \"\\n\".join(chunk['propositions']),\n",
        "            \"current_summary\" : chunk['summary']\n",
        "        }).content\n",
        "\n",
        "        return new_chunk_summary\n",
        "\n",
        "    def _update_chunk_title(self, chunk):\n",
        "        \"\"\"\n",
        "        If you add a new proposition to a chunk, you may want to update the title or else it can get stale\n",
        "        \"\"\"\n",
        "        PROMPT = ChatPromptTemplate.from_messages(\n",
        "            [\n",
        "                (\n",
        "                    \"system\",\n",
        "                    \"\"\"\n",
        "                    You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
        "                    A new proposition was just added to one of your chunks, you should generate a very brief updated chunk title which will inform viewers what a chunk group is about.\n",
        "\n",
        "                    A good title will say what the chunk is about.\n",
        "\n",
        "                    You will be given a group of propositions which are in the chunk, chunk summary and the chunk title.\n",
        "\n",
        "                    Your title should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
        "                    Or month, generalize it to \"date and times\".\n",
        "\n",
        "                    Example:\n",
        "                    Input: Summary: This chunk is about dates and times that the author talks about\n",
        "                    Output: Date & Times\n",
        "\n",
        "                    Only respond with the new chunk title, nothing else.\n",
        "                    \"\"\",\n",
        "                ),\n",
        "                (\"user\", \"Chunk's propositions:\\n{proposition}\\n\\nChunk summary:\\n{current_summary}\\n\\nCurrent chunk title:\\n{current_title}\"),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        runnable = PROMPT | self.llm\n",
        "\n",
        "        updated_chunk_title = runnable.invoke({\n",
        "            \"proposition\": \"\\n\".join(chunk['propositions']),\n",
        "            \"current_summary\" : chunk['summary'],\n",
        "            \"current_title\" : chunk['title']\n",
        "        }).content\n",
        "\n",
        "        return updated_chunk_title\n",
        "\n",
        "    def _get_new_chunk_summary(self, proposition):\n",
        "        PROMPT = ChatPromptTemplate.from_messages(\n",
        "            [\n",
        "                (\n",
        "                    \"system\",\n",
        "                    \"\"\"\n",
        "                    You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
        "                    You should generate a very brief 1-sentence summary which will inform viewers what a chunk group is about.\n",
        "\n",
        "                    A good summary will say what the chunk is about, and give any clarifying instructions on what to add to the chunk.\n",
        "\n",
        "                    You will be given a proposition which will go into a new chunk. This new chunk needs a summary.\n",
        "\n",
        "                    Your summaries should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
        "                    Or month, generalize it to \"date and times\".\n",
        "\n",
        "                    Example:\n",
        "                    Input: Proposition: Greg likes to eat pizza\n",
        "                    Output: This chunk contains information about the types of food Greg likes to eat.\n",
        "\n",
        "                    Only respond with the new chunk summary, nothing else.\n",
        "                    \"\"\",\n",
        "                ),\n",
        "                (\"user\", \"Determine the summary of the new chunk that this proposition will go into:\\n{proposition}\"),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        runnable = PROMPT | self.llm\n",
        "\n",
        "        new_chunk_summary = runnable.invoke({\n",
        "            \"proposition\": proposition\n",
        "        }).content\n",
        "\n",
        "        return new_chunk_summary\n",
        "\n",
        "    def _get_new_chunk_title(self, summary):\n",
        "        PROMPT = ChatPromptTemplate.from_messages(\n",
        "            [\n",
        "                (\n",
        "                    \"system\",\n",
        "                    \"\"\"\n",
        "                    You are the steward of a group of chunks which represent groups of sentences that talk about a similar topic\n",
        "                    You should generate a very brief few word chunk title which will inform viewers what a chunk group is about.\n",
        "\n",
        "                    A good chunk title is brief but encompasses what the chunk is about\n",
        "\n",
        "                    You will be given a summary of a chunk which needs a title\n",
        "\n",
        "                    Your titles should anticipate generalization. If you get a proposition about apples, generalize it to food.\n",
        "                    Or month, generalize it to \"date and times\".\n",
        "\n",
        "                    Example:\n",
        "                    Input: Summary: This chunk is about dates and times that the author talks about\n",
        "                    Output: Date & Times\n",
        "\n",
        "                    Only respond with the new chunk title, nothing else.\n",
        "                    \"\"\",\n",
        "                ),\n",
        "                (\"user\", \"Determine the title of the chunk that this summary belongs to:\\n{summary}\"),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        runnable = PROMPT | self.llm\n",
        "\n",
        "        new_chunk_title = runnable.invoke({\n",
        "            \"summary\": summary\n",
        "        }).content\n",
        "\n",
        "        return new_chunk_title\n",
        "\n",
        "\n",
        "    def _create_new_chunk(self, proposition):\n",
        "        new_chunk_id = str(uuid.uuid4())[:self.id_truncate_limit] # I don't want long ids\n",
        "        new_chunk_summary = self._get_new_chunk_summary(proposition)\n",
        "        new_chunk_title = self._get_new_chunk_title(new_chunk_summary)\n",
        "\n",
        "        self.chunks[new_chunk_id] = {\n",
        "            'chunk_id' : new_chunk_id,\n",
        "            'propositions': [proposition],\n",
        "            'title' : new_chunk_title,\n",
        "            'summary': new_chunk_summary,\n",
        "            'chunk_index' : len(self.chunks)\n",
        "        }\n",
        "        if self.print_logging:\n",
        "            print (f\"Created new chunk ({new_chunk_id}): {new_chunk_title}\")\n",
        "\n",
        "    def get_chunk_outline(self):\n",
        "        \"\"\"\n",
        "        Get a string which represents the chunks you currently have.\n",
        "        This will be empty when you first start off\n",
        "        \"\"\"\n",
        "        chunk_outline = \"\"\n",
        "\n",
        "        for chunk_id, chunk in self.chunks.items():\n",
        "            single_chunk_string = f\"\"\"Chunk ({chunk['chunk_id']}): {chunk['title']}\\nSummary: {chunk['summary']}\\n\\n\"\"\"\n",
        "\n",
        "            chunk_outline += single_chunk_string\n",
        "\n",
        "        return chunk_outline\n",
        "\n",
        "    def _find_relevant_chunk(self, proposition):\n",
        "        current_chunk_outline = self.get_chunk_outline()\n",
        "\n",
        "        PROMPT = ChatPromptTemplate.from_messages(\n",
        "            [\n",
        "                (\n",
        "                    \"system\",\n",
        "                    \"\"\"\n",
        "                    Determine whether or not the \"Proposition\" should belong to any of the existing chunks.\n",
        "\n",
        "                    A proposition should belong to a chunk of their meaning, direction, or intention are similar.\n",
        "                    The goal is to group similar propositions and chunks.\n",
        "\n",
        "                    If you think a proposition should be joined with a chunk, return the chunk id.\n",
        "                    If you do not think an item should be joined with an existing chunk, just return \"No chunks\"\n",
        "\n",
        "                    Example:\n",
        "                    Input:\n",
        "                        - Proposition: \"Greg really likes hamburgers\"\n",
        "                        - Current Chunks:\n",
        "                            - Chunk ID: 2n4l3d\n",
        "                            - Chunk Name: Places in San Francisco\n",
        "                            - Chunk Summary: Overview of the things to do with San Francisco Places\n",
        "\n",
        "                            - Chunk ID: 93833k\n",
        "                            - Chunk Name: Food Greg likes\n",
        "                            - Chunk Summary: Lists of the food and dishes that Greg likes\n",
        "                    Output: 93833k\n",
        "                    \"\"\",\n",
        "                ),\n",
        "                (\"user\", \"Current Chunks:\\n--Start of current chunks--\\n{current_chunk_outline}\\n--End of current chunks--\"),\n",
        "                (\"user\", \"Determine if the following statement should belong to one of the chunks outlined:\\n{proposition}\"),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        runnable = PROMPT | self.llm\n",
        "\n",
        "        chunk_found = runnable.invoke({\n",
        "            \"proposition\": proposition,\n",
        "            \"current_chunk_outline\": current_chunk_outline\n",
        "        }).content\n",
        "\n",
        "        # Pydantic data class\n",
        "        class ChunkID(BaseModel):\n",
        "            \"\"\"Extracting the chunk id\"\"\"\n",
        "            chunk_id: Optional[str]\n",
        "\n",
        "        # Extraction to catch-all LLM responses. This is a bandaid\n",
        "        extraction_chain = create_extraction_chain_pydantic(pydantic_schema=ChunkID, llm=self.llm)\n",
        "        extraction_found = extraction_chain.run(chunk_found)\n",
        "        if extraction_found:\n",
        "            chunk_found = extraction_found[0].chunk_id\n",
        "\n",
        "        # If you got a response that isn't the chunk id limit, chances are it's a bad response or it found nothing\n",
        "        # So return nothing\n",
        "        if len(chunk_found) != self.id_truncate_limit:\n",
        "            return None\n",
        "\n",
        "        return chunk_found\n",
        "\n",
        "    def get_chunks(self, get_type='dict'):\n",
        "        \"\"\"\n",
        "        This function returns the chunks in the format specified by the 'get_type' parameter.\n",
        "        If 'get_type' is 'dict', it returns the chunks as a dictionary.\n",
        "        If 'get_type' is 'list_of_strings', it returns the chunks as a list of strings, where each string is a proposition in the chunk.\n",
        "        \"\"\"\n",
        "        if get_type == 'dict':\n",
        "            return self.chunks\n",
        "        if get_type == 'list_of_strings':\n",
        "            chunks = []\n",
        "            for chunk_id, chunk in self.chunks.items():\n",
        "                chunks.append(\" \".join([x for x in chunk['propositions']]))\n",
        "            return chunks\n",
        "\n",
        "    def pretty_print_chunks(self):\n",
        "        print (f\"\\nYou have {len(self.chunks)} chunks\\n\")\n",
        "        for chunk_id, chunk in self.chunks.items():\n",
        "            print(f\"Chunk #{chunk['chunk_index']}\")\n",
        "            print(f\"Chunk ID: {chunk_id}\")\n",
        "            print(f\"Summary: {chunk['summary']}\")\n",
        "            print(f\"Propositions:\")\n",
        "            for prop in chunk['propositions']:\n",
        "                print(f\"    -{prop}\")\n",
        "            print(\"\\n\\n\")\n",
        "\n",
        "    def pretty_print_chunk_outline(self):\n",
        "        print (\"Chunk Outline\\n\")\n",
        "        print(self.get_chunk_outline())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ac = AgenticChunker()\n",
        "\n",
        "    ## Comment and uncomment the propositions to your hearts content\n",
        "    propositions = [\n",
        "        'The month is October.',\n",
        "        'The year is 2023.',\n",
        "        \"One of the most important things that I didn't understand about the world as a child was the degree to which the returns for performance are superlinear.\",\n",
        "        'Teachers and coaches implicitly told us that the returns were linear.',\n",
        "        \"I heard a thousand times that 'You get out what you put in.'\",\n",
        "        # 'Teachers and coaches meant well.',\n",
        "        # \"The statement that 'You get out what you put in' is rarely true.\",\n",
        "        # \"If your product is only half as good as your competitor's product, you do not get half as many customers.\",\n",
        "        # \"You get no customers if your product is only half as good as your competitor's product.\",\n",
        "        # 'You go out of business if you get no customers.',\n",
        "        # 'The returns for performance are superlinear in business.',\n",
        "        # 'Some people think the superlinear returns for performance are a flaw of capitalism.',\n",
        "        # 'Some people think that changing the rules of capitalism would stop the superlinear returns for performance from being true.',\n",
        "        # 'Superlinear returns for performance are a feature of the world.',\n",
        "        # 'Superlinear returns for performance are not an artifact of rules that humans have invented.',\n",
        "        # 'The same pattern of superlinear returns is observed in fame.',\n",
        "        # 'The same pattern of superlinear returns is observed in power.',\n",
        "        # 'The same pattern of superlinear returns is observed in military victories.',\n",
        "        # 'The same pattern of superlinear returns is observed in knowledge.',\n",
        "        # 'The same pattern of superlinear returns is observed in benefit to humanity.',\n",
        "        # 'In fame, power, military victories, knowledge, and benefit to humanity, the rich get richer.'\n",
        "    ]\n",
        "\n",
        "    ac.add_propositions(propositions)\n",
        "    ac.pretty_print_chunks()\n",
        "    ac.pretty_print_chunk_outline()\n",
        "    print (ac.get_chunks(get_type='list_of_strings'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "1193f8e1-28aa-4c8c-8929-bfbcf7edc618",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "1193f8e1-28aa-4c8c-8929-bfbcf7edc618",
        "outputId": "b70cf104-2b98-4c35-d330-c30942161e0c"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'AgenticChunker' from 'agentic_chunker' (unknown location)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-36111a06409d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# mini script I made\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0magentic_chunker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAgenticChunker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'AgenticChunker' from 'agentic_chunker' (unknown location)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# mini script I made\n",
        "# from agentic_chunker import AgenticChunker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "f550431a-9484-4095-a888-31ec3cbe1b42",
      "metadata": {
        "id": "f550431a-9484-4095-a888-31ec3cbe1b42"
      },
      "outputs": [],
      "source": [
        "ac = AgenticChunker()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "833086aa-d1b7-4ac8-9d85-3a71e6e634ab",
      "metadata": {
        "id": "833086aa-d1b7-4ac8-9d85-3a71e6e634ab"
      },
      "source": [
        "Then let's pass in our propositions to it. There are a lot in the full list so I'm only going to do a subset.\n",
        "\n",
        "This method is slow and expensive, but let's see how the results are.\n",
        "\n",
        "You can turn off the print statements via setting `ac = AgenticChunker(print_logging=False)` when you create your chunker."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "f7e9892a-9a1c-4269-a262-7cc4ab0eed32",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7e9892a-9a1c-4269-a262-7cc4ab0eed32",
        "outputId": "d7a7ec06-fe6f-4166-fe53-4f8b9a63dd4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Adding: 'The return is super linear.'\n",
            "No chunks, creating a new one\n",
            "Created new chunk (89d60): Investment Returns Characteristics\n",
            "\n",
            "Adding: 'The month is October.'\n",
            "No chunks found\n",
            "Created new chunk (d3103): Date & Times\n",
            "\n",
            "Adding: 'The year is 2023.'\n",
            "Chunk Found (d3103), adding to: Date & Times\n",
            "\n",
            "Adding: 'I did not understand one important thing about the world when I was a child.'\n",
            "No chunks found\n",
            "Created new chunk (982da): Childhood Misconceptions\n",
            "\n",
            "Adding: 'The returns for performance are superlinear.'\n",
            "Chunk Found (89d60), adding to: Investment Returns Characteristics\n",
            "\n",
            "Adding: 'Teachers and coaches implicitly told us the returns were linear.'\n",
            "Chunk Found (89d60), adding to: Superlinear Returns in Performance & Investments\n",
            "\n",
            "Adding: 'Teachers and coaches meant well.'\n",
            "No chunks found\n",
            "Created new chunk (107d6): Educational Approaches\n",
            "\n",
            "Adding: 'The statement 'You get out what you put in' was heard a thousand times by the speaker.'\n",
            "No chunks found\n",
            "Created new chunk (9bc5a): Common Sayings & Impact\n",
            "\n",
            "Adding: 'The statement that returns were linear is rarely true.'\n",
            "Chunk Found (89d60), adding to: Performance & Investment Returns Misconceptions\n",
            "\n",
            "Adding: 'If your product is only half as good as your competitor's product, you don't get half as many customers.'\n",
            "Chunk Found (89d60), adding to: Superlinear Returns & Linearity Misconceptions\n",
            "\n",
            "Adding: 'You get no customers if your product is only half as good as your competitor's product.'\n",
            "No chunks found\n",
            "Created new chunk (8efe8): Customer Acquisition & Product Quality\n",
            "\n",
            "Adding: 'You go out of business if you get no customers.'\n",
            "Chunk Found (8efe8), adding to: Customer Acquisition & Product Quality\n",
            "\n",
            "Adding: 'The returns for performance are superlinear in business.'\n",
            "Chunk Found (89d60), adding to: Performance & Business Returns: Linearity vs. Superlinearity\n",
            "\n",
            "Adding: 'Some people believe that superlinear returns for performance are a flaw of capitalism.'\n",
            "Chunk Found (89d60), adding to: Superlinear Returns in Performance & Business\n",
            "\n",
            "Adding: 'Some people believe that changing the rules of capitalism would stop superlinear returns for performance from being true.'\n",
            "Chunk Found (89d60), adding to: Superlinear vs. Linear Returns in Performance, Business & Capitalism\n",
            "\n",
            "Adding: 'Superlinear returns for performance are a feature of the world.'\n",
            "Chunk Found (89d60), adding to: Superlinear Returns & Economic Misconceptions\n",
            "\n",
            "Adding: 'Superlinear returns for performance are not an artifact of rules that humans have invented.'\n",
            "Chunk Found (89d60), adding to: Superlinear Returns & Capitalism Debate\n",
            "\n",
            "Adding: 'The same pattern of superlinear returns is observed in fame.'\n",
            "Chunk Found (89d60), adding to: Superlinear Returns in Performance & Capitalism\n",
            "\n",
            "Adding: 'The same pattern of superlinear returns is observed in power.'\n",
            "Chunk Found (89d60), adding to: Superlinear Returns & Economic Concepts\n",
            "\n",
            "Adding: 'The same pattern of superlinear returns is observed in military victories.'\n",
            "Chunk Found (89d60), adding to: Superlinear Returns in Performance & Society\n",
            "\n",
            "Adding: 'The same pattern of superlinear returns is observed in knowledge.'\n",
            "Chunk Found (89d60), adding to: Superlinear Returns Across Domains\n",
            "\n",
            "Adding: 'The same pattern of superlinear returns is observed in the benefit to humanity.'\n",
            "Chunk Found (89d60), adding to: Concept of Superlinear Returns in Various Fields\n",
            "\n",
            "Adding: 'In all of the areas mentioned, those who are already rich get richer.'\n",
            "Chunk Found (89d60), adding to: Superlinear Returns Across Domains & Their Impact\n"
          ]
        }
      ],
      "source": [
        "ac.add_propositions(essay_propositions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1481fe66-f73c-4fd2-a51c-3a1131a8e57c",
      "metadata": {
        "id": "1481fe66-f73c-4fd2-a51c-3a1131a8e57c"
      },
      "source": [
        "Cool, looks like a few chunks were made. Let's check them out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "27c79f2a-119e-45f2-8883-0f623a7c211e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27c79f2a-119e-45f2-8883-0f623a7c211e",
        "outputId": "7f22880a-fd15-4cd0-9bf0-3a00d6320092"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "You have 6 chunks\n",
            "\n",
            "Chunk #0\n",
            "Chunk ID: 89d60\n",
            "Summary: This chunk explores the concept of superlinear returns, challenging the linear perspective and examining its prevalence and consequences in multiple areas such as business, fame, power, and societal systems.\n",
            "Propositions:\n",
            "    -The return is super linear.\n",
            "    -The returns for performance are superlinear.\n",
            "    -Teachers and coaches implicitly told us the returns were linear.\n",
            "    -The statement that returns were linear is rarely true.\n",
            "    -If your product is only half as good as your competitor's product, you don't get half as many customers.\n",
            "    -The returns for performance are superlinear in business.\n",
            "    -Some people believe that superlinear returns for performance are a flaw of capitalism.\n",
            "    -Some people believe that changing the rules of capitalism would stop superlinear returns for performance from being true.\n",
            "    -Superlinear returns for performance are a feature of the world.\n",
            "    -Superlinear returns for performance are not an artifact of rules that humans have invented.\n",
            "    -The same pattern of superlinear returns is observed in fame.\n",
            "    -The same pattern of superlinear returns is observed in power.\n",
            "    -The same pattern of superlinear returns is observed in military victories.\n",
            "    -The same pattern of superlinear returns is observed in knowledge.\n",
            "    -The same pattern of superlinear returns is observed in the benefit to humanity.\n",
            "    -In all of the areas mentioned, those who are already rich get richer.\n",
            "\n",
            "\n",
            "\n",
            "Chunk #1\n",
            "Chunk ID: d3103\n",
            "Summary: This chunk contains information about specific dates and times, including months and years.\n",
            "Propositions:\n",
            "    -The month is October.\n",
            "    -The year is 2023.\n",
            "\n",
            "\n",
            "\n",
            "Chunk #2\n",
            "Chunk ID: 982da\n",
            "Summary: This chunk contains reflections on misunderstandings or lack of knowledge about the world during childhood.\n",
            "Propositions:\n",
            "    -I did not understand one important thing about the world when I was a child.\n",
            "\n",
            "\n",
            "\n",
            "Chunk #3\n",
            "Chunk ID: 107d6\n",
            "Summary: This chunk contains information about the intentions and attitudes of educators and instructors.\n",
            "Propositions:\n",
            "    -Teachers and coaches meant well.\n",
            "\n",
            "\n",
            "\n",
            "Chunk #4\n",
            "Chunk ID: 9bc5a\n",
            "Summary: This chunk contains information about common sayings or phrases and their impact on the speaker.\n",
            "Propositions:\n",
            "    -The statement 'You get out what you put in' was heard a thousand times by the speaker.\n",
            "\n",
            "\n",
            "\n",
            "Chunk #5\n",
            "Chunk ID: 8efe8\n",
            "Summary: This chunk explores the consequences of product quality on business viability and customer acquisition.\n",
            "Propositions:\n",
            "    -You get no customers if your product is only half as good as your competitor's product.\n",
            "    -You go out of business if you get no customers.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ac.pretty_print_chunks()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4aebf75-53b9-4695-bea9-effa0adf967e",
      "metadata": {
        "id": "e4aebf75-53b9-4695-bea9-effa0adf967e"
      },
      "source": [
        "Awesome, then if we wanted to get the chunks properly, then we get extract a list of strings with them. The chunks propositions will be joined in the same string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "e90e5eb4-676f-4310-8bbf-be87f94d07eb",
      "metadata": {
        "id": "e90e5eb4-676f-4310-8bbf-be87f94d07eb"
      },
      "outputs": [],
      "source": [
        "chunks = ac.get_chunks(get_type='list_of_strings')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "d08eb17e-10ab-426e-8998-bed29ab76dac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d08eb17e-10ab-426e-8998-bed29ab76dac",
        "outputId": "da96ed82-c99d-43b8-ee97-ebaa7c92187a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"The return is super linear. The returns for performance are superlinear. Teachers and coaches implicitly told us the returns were linear. The statement that returns were linear is rarely true. If your product is only half as good as your competitor's product, you don't get half as many customers. The returns for performance are superlinear in business. Some people believe that superlinear returns for performance are a flaw of capitalism. Some people believe that changing the rules of capitalism would stop superlinear returns for performance from being true. Superlinear returns for performance are a feature of the world. Superlinear returns for performance are not an artifact of rules that humans have invented. The same pattern of superlinear returns is observed in fame. The same pattern of superlinear returns is observed in power. The same pattern of superlinear returns is observed in military victories. The same pattern of superlinear returns is observed in knowledge. The same pattern of superlinear returns is observed in the benefit to humanity. In all of the areas mentioned, those who are already rich get richer.\",\n",
              " 'The month is October. The year is 2023.',\n",
              " 'I did not understand one important thing about the world when I was a child.',\n",
              " 'Teachers and coaches meant well.',\n",
              " \"The statement 'You get out what you put in' was heard a thousand times by the speaker.\",\n",
              " \"You get no customers if your product is only half as good as your competitor's product. You go out of business if you get no customers.\"]"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e920b345-0294-4b5d-995c-da36bdfaeace",
      "metadata": {
        "id": "e920b345-0294-4b5d-995c-da36bdfaeace"
      },
      "source": [
        "Great, now we can go use that in our evaluations for your retrieval."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75a8462b-8c10-4550-b42c-453ecdec3647",
      "metadata": {
        "id": "75a8462b-8c10-4550-b42c-453ecdec3647"
      },
      "source": [
        "## Bonus Level: Alternative Representation <a id=\"BonusLevel\"></a>\n",
        "So far I've shown how to chunk up your raw text (okay, I was a bit liberal with level 5).\n",
        "\n",
        "But what if your raw text isn't the best way to represent your data for your task?\n",
        "\n",
        "For example, if you're doing semantic search on chat messages, raw chat messages may lack the context to make a successful embedding. Maybe actually trying to semantic search of a summary of a conversation would do better. Or maybe hypothetical questions that the chat would answer?\n",
        "\n",
        "This is where the world of chunking/splitting starts to dive into the world of [indexing](https://docs.llamaindex.ai/en/stable/understanding/indexing/indexing.html#what-is-an-index). When you index, you're making a choice about how you want to represent your data in your data base or knowledge base.\n",
        "\n",
        "This is more of a retrieval topic, but it's worth talking about with chunking.\n",
        "\n",
        "Let's quickly go through a few popular alternative ways developers like to represent their data. There are unlimited methods to try. We'll review 4 of them\n",
        "\n",
        "* **[Multi-Vector Indexing](https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector)** - This is when you do semantic search for a vector that is derived from something other than your raw text\n",
        "    * **Summaries** - A summary of your chunk\n",
        "    * **Hypothetical questions** - Good for chat messages used as knowledge base\n",
        "    * **Child Documents** - Parent Document Retriever\n",
        "* **Graph Based Chunking** - Transposing your raw text into a graph structure\n",
        "\n",
        "### Summaries\n",
        "Instead of embedding your raw text, embed a summary of your raw text which will have more dense information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "fce9678d-187e-4293-b1aa-eda9045035ee",
      "metadata": {
        "id": "fce9678d-187e-4293-b1aa-eda9045035ee"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
        "from langchain.storage import InMemoryByteStore\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b445bc40-531b-4a65-b4ee-852377862ce9",
      "metadata": {
        "id": "b445bc40-531b-4a65-b4ee-852377862ce9"
      },
      "source": [
        "Let's use our Super Linear essay again. I'll split it into large chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "ca8c1a25-aaff-4d74-a5fd-f681f7cf478f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca8c1a25-aaff-4d74-a5fd-f681f7cf478f",
        "outputId": "e1e9dde6-2150-421a-e04b-30bad81392ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You have 6 docs\n"
          ]
        }
      ],
      "source": [
        "with open('./Super-linear-return.txt') as file:\n",
        "    essay = file.read()\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=4000, chunk_overlap=0)\n",
        "\n",
        "docs = splitter.create_documents([essay])\n",
        "\n",
        "print (f\"You have {len(docs)} docs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b418bdb-c03c-4885-8a74-983c5732f6dc",
      "metadata": {
        "id": "3b418bdb-c03c-4885-8a74-983c5732f6dc"
      },
      "source": [
        "Spin up a chain that will quickly summarize for you"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "460da5b4-cd95-4ea5-ae21-57c2cf99b80f",
      "metadata": {
        "id": "460da5b4-cd95-4ea5-ae21-57c2cf99b80f"
      },
      "outputs": [],
      "source": [
        "chain = (\n",
        "    {\"doc\": lambda x: x.page_content}\n",
        "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
        "    | ChatOpenAI(max_retries=0)\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df67afe1-694f-47a5-acea-7a788ea5ba07",
      "metadata": {
        "id": "df67afe1-694f-47a5-acea-7a788ea5ba07"
      },
      "source": [
        "Then let's get the summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "d6473c02-5728-418d-a201-5216d83599dd",
      "metadata": {
        "id": "d6473c02-5728-418d-a201-5216d83599dd"
      },
      "outputs": [],
      "source": [
        "summaries = chain.batch(docs, {\"max_concurrency\": 5})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9282853-9a62-4ffd-a1bf-b0e8853ed864",
      "metadata": {
        "id": "f9282853-9a62-4ffd-a1bf-b0e8853ed864"
      },
      "source": [
        "Let's look at a sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "6ecc0c8f-3eb5-4e42-bd6e-2fdec56cc5b1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "6ecc0c8f-3eb5-4e42-bd6e-2fdec56cc5b1",
        "outputId": "4ef4e3fe-e0fb-4844-93f1-0c39cd367a6d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"The document discusses the concept of superlinear returns, which refers to the idea that the returns for performance are not linear, but rather increase exponentially. The author explains that this concept is important to understand in various aspects of life, including business, fame, power, military victories, knowledge, and benefit to humanity. The document identifies two fundamental causes of superlinear returns: exponential growth and thresholds. It provides examples of exponential growth in bacterial cultures and startups. The document also mentions Y Combinator's emphasis on growth rate for startups and how it aligns with the concept of superlinear returns. It concludes by highlighting that exponential growth is not something that comes naturally to humans and that there are limited customs or experiences to guide us in dealing with it.\""
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summaries[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9aaa4786-1d43-463b-a63d-55e1a9f35af3",
      "metadata": {
        "id": "9aaa4786-1d43-463b-a63d-55e1a9f35af3"
      },
      "source": [
        "Then we are going to create a vectorstore (holds vectors + summaries) and a docstore (holds raw docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "ZJqKNwLxG1aG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJqKNwLxG1aG",
        "outputId": "11fe108f-3000-48ec-d441-0995bbfe2b6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting chromadb\n",
            "  Downloading chromadb-0.4.22-py3-none-any.whl (509 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.0/509.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.0.3)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.10.13)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.109.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.25.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.23.5)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.3.1-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.9.0)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.15.1)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.22.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.22.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.43b0-py3-none-any.whl (11 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.22.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.6/105.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.0)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.1.1)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.60.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (698 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.2.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-4.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=19.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (23.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
            "Collecting starlette<0.36.0,>=0.35.0 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.35.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2023.11.17)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.17.3)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
            "Collecting importlib-metadata<7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: backoff<3.0.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.62.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.22.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.22.0-py3-none-any.whl (17 kB)\n",
            "Collecting opentelemetry-proto==1.22.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.22.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-instrumentation-asgi==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.43b0-py3-none-any.whl (14 kB)\n",
            "Collecting opentelemetry-instrumentation==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.43b0-py3-none-any.whl (28 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.43b0-py3-none-any.whl (36 kB)\n",
            "Collecting opentelemetry-util-http==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.43b0-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (67.7.2)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.43b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.7.2-py3-none-any.whl (24 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.6)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.20.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.36.0,>=0.35.0->fastapi>=0.95.2->chromadb) (3.7.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.36.0,>=0.35.0->fastapi>=0.95.2->chromadb) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.36.0,>=0.35.0->fastapi>=0.95.2->chromadb) (1.2.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.1)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=bf4f95ea1868567bae0409279fabc583adf01da45ebfb59016f3bf5b0866fdb2\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, mmh3, websockets, uvloop, uvicorn, pulsar-client, overrides, opentelemetry-util-http, opentelemetry-semantic-conventions, opentelemetry-proto, importlib-metadata, httptools, chroma-hnswlib, bcrypt, asgiref, watchfiles, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, opentelemetry-sdk, opentelemetry-instrumentation, kubernetes, fastapi, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 7.0.1\n",
            "    Uninstalling importlib-metadata-7.0.1:\n",
            "      Successfully uninstalled importlib-metadata-7.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asgiref-3.7.2 bcrypt-4.1.2 chroma-hnswlib-0.7.3 chromadb-0.4.22 fastapi-0.109.0 httptools-0.6.1 importlib-metadata-6.11.0 kubernetes-29.0.0 mmh3-4.1.0 monotonic-1.6 opentelemetry-api-1.22.0 opentelemetry-exporter-otlp-proto-common-1.22.0 opentelemetry-exporter-otlp-proto-grpc-1.22.0 opentelemetry-instrumentation-0.43b0 opentelemetry-instrumentation-asgi-0.43b0 opentelemetry-instrumentation-fastapi-0.43b0 opentelemetry-proto-1.22.0 opentelemetry-sdk-1.22.0 opentelemetry-semantic-conventions-0.43b0 opentelemetry-util-http-0.43b0 overrides-7.4.0 posthog-3.3.1 pulsar-client-3.4.0 pypika-0.48.9 starlette-0.35.1 uvicorn-0.25.0 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n"
          ]
        }
      ],
      "source": [
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "cc76c297-804f-4e5a-9fed-f28b88f57059",
      "metadata": {
        "id": "cc76c297-804f-4e5a-9fed-f28b88f57059"
      },
      "outputs": [],
      "source": [
        "# The vectorstore to use to index the child chunks\n",
        "vectorstore = Chroma(collection_name=\"summaries\", embedding_function=OpenAIEmbeddings())\n",
        "\n",
        "# The storage layer for the parent documents\n",
        "store = InMemoryByteStore()\n",
        "\n",
        "id_key = \"doc_id\"\n",
        "\n",
        "# The retriever (empty to start)\n",
        "retriever = MultiVectorRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    byte_store=store,\n",
        "    id_key=id_key,\n",
        ")\n",
        "doc_ids = [str(uuid.uuid4()) for _ in docs]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63f44d2b-6a25-431c-be0b-72fff86e019c",
      "metadata": {
        "id": "63f44d2b-6a25-431c-be0b-72fff86e019c"
      },
      "source": [
        "Then you want to create documents out of your summary list, add the doc_id to it's metadata. This will tie it back to the original document so you know which summary goes with which original doc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "032d5cb8-cfe5-4a89-bb9a-fb2872e41be3",
      "metadata": {
        "id": "032d5cb8-cfe5-4a89-bb9a-fb2872e41be3"
      },
      "outputs": [],
      "source": [
        "summary_docs = [\n",
        "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
        "    for i, s in enumerate(summaries)\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02878000-5341-42fc-a1b0-a7e9da3367d7",
      "metadata": {
        "id": "02878000-5341-42fc-a1b0-a7e9da3367d7"
      },
      "source": [
        "Then add them both to your vectorestore and docstore. When you add the docs to the vectorstore it will get the embeddings for them too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "53351f4d-f5db-4aeb-9b79-f4a0a3ba30f6",
      "metadata": {
        "id": "53351f4d-f5db-4aeb-9b79-f4a0a3ba30f6"
      },
      "outputs": [],
      "source": [
        "# Adds the summaries\n",
        "retriever.vectorstore.add_documents(summary_docs)\n",
        "\n",
        "# Adds the raw documents\n",
        "retriever.docstore.mset(list(zip(doc_ids, docs)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e761a8ae-d31c-483b-bdf4-4a625e3e9662",
      "metadata": {
        "id": "e761a8ae-d31c-483b-bdf4-4a625e3e9662"
      },
      "source": [
        "Then if you want you can add the original docs to the vectorstore as well. Just make sure to add the doc id to it as well so you can tie it all together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5c3b781-3166-4d91-ad03-48bc8b246d61",
      "metadata": {
        "id": "d5c3b781-3166-4d91-ad03-48bc8b246d61"
      },
      "outputs": [],
      "source": [
        "# for i, doc in enumerate(docs):\n",
        "#     doc.metadata[id_key] = doc_ids[i]\n",
        "# retriever.vectorstore.add_documents(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3397531-c8d3-4037-8f64-c51eeb6dc49b",
      "metadata": {
        "id": "b3397531-c8d3-4037-8f64-c51eeb6dc49b"
      },
      "source": [
        "Great, now that we've done all that work, let's try a search. If you run the code below, you'll search on the summary embeddings, but you'll get the raw documents returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2b9848e-136b-496f-b3ba-f57720a11993",
      "metadata": {
        "id": "b2b9848e-136b-496f-b3ba-f57720a11993"
      },
      "outputs": [],
      "source": [
        "# retriever.get_relevant_documents(query=\"the concept of superlinear returns, which refers to the idea that the returns for performance are not linear\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7d6ab29-a89a-4259-847d-492ccbaad02f",
      "metadata": {
        "id": "e7d6ab29-a89a-4259-847d-492ccbaad02f"
      },
      "source": [
        "### Hypothetical Questions\n",
        "You can generate hypothetical questions about your raw documents. Check out [LangChain's](https://python.langchain.com/docs/modules/data_connection/retrievers/multi_vector#hypothetical-queries) implementation of it for more information.\n",
        "\n",
        "This is espeically helpful when you have sparse unstructured data, like chat messages.\n",
        "\n",
        "Say you were to build a bot that uses slack conversations as a knowledge base. Trying to do semantic search on raw chat messages might not have the greatest results. However, if you were to generate hypothetical questions that the slack messages would answer, then when you get a new question in, you'll likely have a better chance matching.\n",
        "\n",
        "The code for this will be the same as the summary code, but instead of asking the LLM to make a summary, you'll ask it for hypothetical questions.\n",
        "\n",
        "### Parent Document Retriever (PDR)\n",
        "Much like the previous two, Parent Document Retriever builds on the concept of doing semantic search on a varied representation of your data.\n",
        "\n",
        "The hypothesis with the PDR is that smaller chunks have a higher likely hood of being matched semantically with a potential query. However, those smaller chunks may not have all the context they need, so instead of passing the smaller chunks to your LLM, get the parent chunk of the smaller chunk. This means you get a larger chunk which the smaller chunk is placed in.\n",
        "\n",
        "Check out LangChain's implementation's implementation of it [here](https://python.langchain.com/docs/modules/data_connection/retrievers/parent_document_retriever).\n",
        "\n",
        "I have a full tutorial on it at [FullStackRetrieval.com](https://fullstackretrieval.com/) if you want to go deeper on that.\n",
        "\n",
        "I want to quickly go over a similar method in [Llama Index](https://www.llamaindex.ai/) with their `HierarchicalNodeParser` which will split a document at various chunk sizes (there will be a bunch of overlaps but that is the purpose). When combined with their `AutoMergingRetriever` you can do complicated retrieval easily. Their walkthrough [here](https://docs.llamaindex.ai/en/stable/examples/retrievers/auto_merging_retriever.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "d078afc3-ecea-4cf1-b430-e27837508e86",
      "metadata": {
        "id": "d078afc3-ecea-4cf1-b430-e27837508e86"
      },
      "outputs": [],
      "source": [
        "from llama_index.node_parser import HierarchicalNodeParser\n",
        "\n",
        "node_parser = HierarchicalNodeParser.from_defaults(\n",
        "    chunk_sizes=[2048, 512, 128],\n",
        "    chunk_overlap=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rYi2HBaXHXUh",
      "metadata": {
        "id": "rYi2HBaXHXUh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "11c19a08-6a94-45a4-8d31-1ad91e014030",
      "metadata": {
        "id": "11c19a08-6a94-45a4-8d31-1ad91e014030"
      },
      "outputs": [],
      "source": [
        "from llama_index import SimpleDirectoryReader\n",
        "\n",
        "documents = SimpleDirectoryReader(\n",
        "    input_files=[\"./startups.txt\"]\n",
        ").load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26a9ecb0-c264-40cc-938a-3610c1a31835",
      "metadata": {
        "id": "26a9ecb0-c264-40cc-938a-3610c1a31835"
      },
      "source": [
        "Then let's do our splitting. There will be a bunch of chunks since we included `128` as a chunk size above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "9806e1a0-79df-418d-962d-3757081fd273",
      "metadata": {
        "id": "9806e1a0-79df-418d-962d-3757081fd273"
      },
      "outputs": [],
      "source": [
        "nodes = node_parser.get_nodes_from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "8ad26bbc-1b67-45dc-b44e-d62e36fd3589",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ad26bbc-1b67-45dc-b44e-d62e36fd3589",
        "outputId": "33168abf-ec37-432f-b2fb-1288bac7427b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You have 105 nodes\n"
          ]
        }
      ],
      "source": [
        "print (f\"You have {len(nodes)} nodes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d703f821-f2e2-4975-a904-893007bb256c",
      "metadata": {
        "id": "d703f821-f2e2-4975-a904-893007bb256c"
      },
      "source": [
        "Then let's look at the relationships that are available to one of the small nodes at the end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "c037ce3e-d480-43e6-be47-0e7415153480",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c037ce3e-d480-43e6-be47-0e7415153480",
        "outputId": "3aa07e26-ac26-4a89-b2be-8345452e8c98"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='0b042fda-4110-4d6f-880d-5bdf4b6321d3', node_type=<ObjectType.TEXT: '1'>, metadata={'file_path': 'startups.txt', 'file_name': 'startups.txt', 'file_type': 'text/plain', 'file_size': 36019, 'creation_date': '2024-01-15', 'last_modified_date': '2024-01-15', 'last_accessed_date': '2024-01-15'}, hash='abd0dd8791460298261be652207f7dc41088134896494bed71943e7a32d2e240'),\n",
              " <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='1959e8b6-6fd4-4595-8c93-b0d08d6324bb', node_type=<ObjectType.TEXT: '1'>, metadata={'file_path': 'startups.txt', 'file_name': 'startups.txt', 'file_type': 'text/plain', 'file_size': 36019, 'creation_date': '2024-01-15', 'last_modified_date': '2024-01-15', 'last_accessed_date': '2024-01-15'}, hash='7a5554a08e8af2887a1a44f8187c47f0981757473e2456f7b154360bec7d1b32'),\n",
              " <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='78eb8835-c72c-4ce0-994f-abdbf3a25e80', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='8946971ce102a3d3214cd7178405818505d9527c7dbca4219d53c70829c73dc0'),\n",
              " <NodeRelationship.PARENT: '4'>: RelatedNodeInfo(node_id='0b042fda-4110-4d6f-880d-5bdf4b6321d3', node_type=<ObjectType.TEXT: '1'>, metadata={'file_path': 'startups.txt', 'file_name': 'startups.txt', 'file_type': 'text/plain', 'file_size': 36019, 'creation_date': '2024-01-15', 'last_modified_date': '2024-01-15', 'last_accessed_date': '2024-01-15'}, hash='abd0dd8791460298261be652207f7dc41088134896494bed71943e7a32d2e240')}"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nodes[-2].relationships"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6fb0ed8-4427-4e8d-a8e9-9d11f3dcef9a",
      "metadata": {
        "id": "a6fb0ed8-4427-4e8d-a8e9-9d11f3dcef9a"
      },
      "source": [
        "You can see there are source, previous, next, and parent. For [more information on these](https://docs.llamaindex.ai/en/stable/api/llama_index.schema.NodeRelationship.html).\n",
        "\n",
        "### Graph Structure\n",
        "If your data is rich with entities, relationships, and connections, then a graph structure may be best for you.\n",
        "\n",
        "Few options:\n",
        "* [Diffbot](https://www.diffbot.com/)\n",
        "* [InstaGraph](https://github.com/yoheinakajima/instagraph) - By [Yohei](https://twitter.com/yoheinakajima)\n",
        "\n",
        "I'll run through the LangChain supported version of Diffbot due to brevity. You'll need an API key from DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "Pv2momkZHoLI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pv2momkZHoLI",
        "outputId": "5b557fbb-687a-416e-fcf7-56e764c1a838"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.0)\n",
            "Collecting langchain-experimental\n",
            "  Downloading langchain_experimental-0.0.49-py3-none-any.whl (165 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.7/165.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.7.2)\n",
            "Collecting neo4j\n",
            "  Downloading neo4j-5.16.0.tar.gz (197 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.8/197.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.24)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.12)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.10)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.77 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.80)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.26.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from neo4j) (2023.3.post1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.7->langchain) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Building wheels for collected packages: neo4j, wikipedia\n",
            "  Building wheel for neo4j (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neo4j: filename=neo4j-5.16.0-py3-none-any.whl size=273811 sha256=ef1649592da0c3840aa25971078a0474760e07aa2fd2b69be94f80299660025b\n",
            "  Stored in directory: /root/.cache/pip/wheels/20/a0/f6/87a1ec9636c915fe2d6c6e859fd55a6231dd9bc95a1d5394b1\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=7c2c567d8f02f7650ef0546eaea184018039b47afa4c9d79fbaa2ea5104313b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built neo4j wikipedia\n",
            "Installing collected packages: neo4j, wikipedia, langchain-experimental\n",
            "Successfully installed langchain-experimental-0.0.49 neo4j-5.16.0 wikipedia-1.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-experimental openai neo4j wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "547ee2d2-f852-4f01-bc1b-ec5355f36e4c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "547ee2d2-f852-4f01-bc1b-ec5355f36e4c",
        "outputId": "b7d8bbe8-d56e-4ff1-84d8-b3694dc138c2"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain_experimental'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-90cbe7bc5551>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# !pip3 install langchain langchain-experimental openai neo4j wikipedia\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_experimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiffbot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDiffbotGraphTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdiffbot_nlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiffbotGraphTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiffbot_api_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DIFFBOT_API_KEY\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'YourKey'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_experimental'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# !pip3 install langchain langchain-experimental openai neo4j wikipedia\n",
        "\n",
        "from langchain_experimental.graph_transformers.diffbot import DiffbotGraphTransformer\n",
        "\n",
        "diffbot_nlp = DiffbotGraphTransformer(diffbot_api_key=os.getenv(\"DIFFBOT_API_KEY\", 'YourKey'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e63a7ffb-acdc-4fd3-b4ba-9ce4eb09bcfa",
      "metadata": {
        "id": "e63a7ffb-acdc-4fd3-b4ba-9ce4eb09bcfa"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "Greg is friends with Bobby. San Francisco is a great city, but New York is amazing.\n",
        "Greg lives in New York.\n",
        "\"\"\"\n",
        "docs = [Document(page_content=text)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41436339-39c6-4cf7-aba6-0a2dcafec00b",
      "metadata": {
        "id": "41436339-39c6-4cf7-aba6-0a2dcafec00b",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "graph_documents = diffbot_nlp.convert_to_graph_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "250215f0-2ba9-49e4-8aef-1751ccc1b1bf",
      "metadata": {
        "id": "250215f0-2ba9-49e4-8aef-1751ccc1b1bf"
      },
      "outputs": [],
      "source": [
        "graph_documents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "949e24e2-1d32-4ff3-a16e-ada98e46d6be",
      "metadata": {
        "id": "949e24e2-1d32-4ff3-a16e-ada98e46d6be"
      },
      "source": [
        "## Wrap up\n",
        "\n",
        "Congratulations on making it to the end of this video. The aim was to educate you on the chunking theory, give a nod to retrieval, and encourage you to try out these methods on your data.\n",
        "\n",
        "I always like hearing what you think about the code, video or how you use this in your role. Let me know on [twitter](https://twitter.com/GregKamradt) or email (contact@dataindependent.com)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
